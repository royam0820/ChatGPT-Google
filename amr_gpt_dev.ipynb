{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/royam0820/ChatGPT-Google/blob/main/amr_gpt_dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJpXpmjEYC_T"
      },
      "source": [
        "# Building a GPT\n",
        "\n",
        "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoo37bwDGD2f"
      },
      "source": [
        "ChatGPT is interesting. It sequentially generates text based on prompts. And it does so slightly differently every time.<br>Also, its prompt acceptance technically seems to not be limited by anything.<br>\n",
        "\n",
        "**ChatGPT is a probabilistic system, a language model**.<br>\n",
        "**It continues a sequence started by our prompt by modeling a continuing sequence of words.**\n",
        "\n",
        "How does this work? What kind of model is applied under the hood?<br>\n",
        "[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) proposed the Transformer model architecture.<br>\n",
        "A transformer-based language model is a type of neural network architecture that is used for natural languages processing tasks such as language translation, text summarization, and language generation. The key innovation of the transformer architecture is the **attention mechanism**, which allows the model to weigh the importance of different parts of the input when making predictions.\n",
        "\n",
        "Transformers really took over the field of AI by now..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCKFIEqdZr_d"
      },
      "source": [
        "## Objective\n",
        "**We will train a transformer-based, character-level language model** on [Tiny-Shakespeare](https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt) (all of Shakespeare in a single file).\n",
        "\n",
        "Given a chunk of text from [Tiny Shakespeare](https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt), the transformer will decide on what character will follow.\n",
        "GPT is state-of-the-art (2022) in language modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HSklX08G76-",
        "outputId": "ab09b21f-c14e-4ddd-b90a-f7e5ef45e90b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nZ3LC6LvZf0y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joKpVCV-e1zE"
      },
      "source": [
        "# The Dataset\n",
        "\n",
        "Let's first look at the contents of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5hjCcLDr2WC",
        "outputId": "2fbee04f-da79-480a-9515-c83a28187aa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-16 21:15:11--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-03-16 21:15:11 (20.0 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset - file input.txt\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ad5mM9lamYm"
      },
      "source": [
        "NB: our small dataset contains Shakespeare texts contained into a file called `input.txt` of size ! MB. We are dealing with roughly 1 million characters. We will use this file to model how these characters follow each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "76a81894-f433-4054-bd4c-39efb319e5a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "45ae226a-3aed-4f84-a06a-5d80f08f69df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "db6921bf-1619-4014-9ee5-8bdf80afdac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocabulary size {65}\n"
          ]
        }
      ],
      "source": [
        "# finding the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))   # Get all unique characters in the text\n",
        "vocab_size = len(chars)           # Length of the vocabulary (this includes the space character)\n",
        "print(''.join(chars))             # joins all the characters in chars back into a single string\n",
        "print(f'vocabulary size', {vocab_size})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3vJns4aTdnv"
      },
      "source": [
        "NB: **The vocabulary size is a total of 65 characters for the text variable, apace included.**\n",
        "\n",
        "`chars = sorted(list(set(text)))` This line performs several operations on `text`:\n",
        "- `set(text)`: Converts the string text into a set of its unique characters. A set is a collection that automatically removes duplicates, so after this operation, each character from text will appear only once.\n",
        "- `list(set(text))`: Converts the set of unique characters back into a list. This is necessary because a set does not preserve order, and we might want to work with the characters in a specific sequence.\n",
        "- `sorted(list(set(text)))`: Sorts the list of unique characters. This ensures that the characters are in a consistent order, typically alphabetical for strings. The result is assigned to the variable chars.\n",
        "\n",
        "`print(''.join(chars))`: This line joins all the characters in chars back into a single string (with no spaces between them) and prints it. This shows what unique characters are present in the text, in sorted order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh7MnG09q8nN"
      },
      "source": [
        "## Tokenization Process - using the encoder and decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "8bddcb69-1a22-4d1a-9eb9-f37ec41e8306"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from characters to integers and vice-versa\n",
        "# building a look-up table via a dictionary\n",
        "stoi = { ch:i for i,ch in enumerate(chars) } # Character to index mapping\n",
        "itos = { i:ch for i,ch in enumerate(chars) } # Index to character mapping\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]           # encode a string to a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l])  # Decode a list of integers to a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9gfxH5Ek-60"
      },
      "source": [
        "NB: this is a **very simple encoding/decoding procedure, in practice, people used subword units tokenizer**. Google, for example, uses a [sentencepiece](https://github.com/google/sentencepiece\n",
        ") schema for text tokenizer and detokenizer. SentencePiece implements **subword units** (e.g., **byte-pair-encoding (BPE)** meaning you are not coding entire words but you are not also encoding individual characters.  OpenAI has its library called [tiktoken](https://github.com/openai/tiktoken), it is a **fast BPE tokeniser** for use with OpenAI's models. It is efficient API usage. It helps developers estimate API usage costs by counting tokens in text and supports automatic loading for model-specific encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u5wGWwbZJAB"
      },
      "source": [
        "## Digression - Tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kYq-dIOZJAB"
      },
      "source": [
        "Different systems use different approaches to encoding/decoding.<br>\n",
        "For example, OpenAI uses byte-pair encoding (BPE) with their GPT-2 model.<br>\n",
        "BPE is a subword tokenization technique. It is a bit more complex than what we will do here, but its shown here nonetheless for a little bit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y93h_Z2qZJAB",
        "outputId": "839fd24c-7f81-437f-e422-8751dc73a0d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[71, 4178, 612]\n",
            "hii there\n",
            "50257\n"
          ]
        }
      ],
      "source": [
        "enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "msg = \"hii there\"\n",
        "token_list = enc.encode(msg)\n",
        "print(token_list) # BPE returns fewer tokens than the character encoding\n",
        "print(enc.decode(enc.encode(\"hii there\")))\n",
        "\n",
        "print(enc.n_vocab) # total amount of tokens in the vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPJ9oxl8ZJAB"
      },
      "source": [
        "NB: Tiktoken shows that there is a trade-off between the length of the encoding and the amount of tokens.<br>\n",
        "**We can have short sequences of tokens with very large vocabulary, or we can just as well have long sequences of tokens with a small vocabulary**.\n",
        "\n",
        "This BPE approach is used widely for NLP tasks nowadays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TSuTS8XriCZ"
      },
      "source": [
        "## Tokenizing the Entire Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le6Xg9_hrqs7",
        "outputId": "52125435-b81d-42d1-da06-14ee0c7e393c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size: torch.Size([1115394]) elements of type torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(f'Total size: {data.shape} elements of type {data.dtype}')\n",
        "print(data[:1000]) # the 1000 characters we looked at earlier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO0SSy-Inc6w",
        "outputId": "d93bb046-9f82-405e-cba3-b02a5c245df8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jnMYMGRs5o3"
      },
      "source": [
        "NB: this output is the tokenization, character by character, of our 1000 characters text. For instance, `0` is a new line character and `1` is a space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdIQI6VXtsrv"
      },
      "source": [
        "## Splitting the Dataset\n",
        "\n",
        "The entire Tiny-Shakespeare text is now represented as a sequence of integers.\n",
        "We can start separating the data into training and validation sets.\n",
        "\n",
        "The **split** between a **training and a validation dataset** is a fundamental concept in machine learning and data science, aimed at creating robust and generalizable models. Here's an overview of what it means and why it's important:\n",
        "\n",
        "**Training Dataset**: This is the subset of the data that we use **to train our machine learning model**. The model learns to make predictions or decisions based on this data. The training process involves adjusting the model's parameters to minimize the error between the predicted outputs and the actual outcomes in the training dataset.\n",
        "\n",
        "**Validation Dataset**: This subset of the data is used **to evaluate the model's performance during the training phase**. It acts as a proxy for test data, helping to tune the hyperparameters (settings of the model that are fixed before the training process begins, like learning rate or the depth of a decision tree) and to prevent overfitting. Overfitting occurs when a model learns the training data too well, including its noise and outliers, making it perform poorly on unseen data because it has essentially memorized the training dataset rather than learned the underlying patterns.\n",
        "\n",
        "The primary purposes of creating a split between training and validation datasets include:\n",
        "\n",
        "**Model Evaluation**: The validation dataset provides a reliable estimate of the performance of the model on new, unseen data. This helps in evaluating how well the model has learned from the training dataset and how it generalizes to data it hasn't seen before.\n",
        "\n",
        "**Hyperparameter Tuning**: The validation set is crucial for tuning the model's hyperparameters. By evaluating the model's performance on the validation set, one can adjust the hyperparameters to find the best combination that maximizes the model's performance.\n",
        "\n",
        "**Preventing Overfitting**: Regularly checking the model's performance on the validation set during training can signal if the model is starting to memorize the training data rather than learning general patterns. If the model's performance on the training set improves while its performance on the validation set worsens, it's likely overfitting.\n",
        "\n",
        "A typical **workflow** involves iteratively training the model on the training dataset, assessing its performance on the validation dataset, and adjusting the model or its hyperparameters based on this assessment. After finalizing the model, its performance is then tested on a separate, untouched dataset known as the test dataset to evaluate its real-world applicability.\n",
        "\n",
        "The **split ratio** between the training and validation datasets can vary depending on the total size of the dataset and the specific problem or domain, but common splits include **70/30, 80/20**, or using techniques like k-fold cross-validation for more efficient use of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PaPwEEkpurBc"
      },
      "outputs": [],
      "source": [
        "# splitting the data between train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest 10% will be val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9BuKGIPvhmp",
        "outputId": "a3c0728d-a990-48f5-b394-7b7469963abc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length training set: {1003854}\n",
            "length validation set: {111540}\n"
          ]
        }
      ],
      "source": [
        "# amr\n",
        "print(f\"length training set:\", {len(train_data)})\n",
        "print(f\"length validation set:\", {len(val_data)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53Xp-1g9_hfT"
      },
      "source": [
        "### Block Size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgcgA_33wMqL"
      },
      "source": [
        "We will now sample **random chunks out of the training set** and train them chunks at the time. Feeding the NN with the whole training set in one shot will not be computationally feasible and too expensive. **Chunks are basically the length of a sequence which is called a block size**.\n",
        "\n",
        "Block size in NLP tasks refers to the number of tokens (words or characters) that a model can process in a single input.\n",
        "\n",
        "For models like Transformers, the **block size is critical** because it determines the **sequence length that the model can handle at once**, affecting both the computational resources required and the model's ability to capture long-distance dependencies in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ImXFvhoZJAC"
      },
      "source": [
        "Let's prepare the model. We will never feed our model the entire sequence of tokens as prompt at once.<br>\n",
        "Instead, we will feed it **a randomly drawn but consecutive sequence of tokens**.<br>\n",
        "The model will then predict the next token in the sequence from this prompt.<br>\n",
        "\n",
        "> We call these consecutive, size-limited input sequences of tokens **blocks**.<br>\n",
        "> Size-limited means that blocks can have a length of up to `block_size`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex9Ii3aryveW",
        "outputId": "875e4cd3-e621-4c4c-9b1c-f17cdf1b2e9f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "block_size = 8              # Upper limit on the length of the text sequences\n",
        "train_data[:block_size+1]   # First 9 characters (8 + 1 for the target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naNz1lqvzUPV"
      },
      "source": [
        "NB: this is the first 9 characters in the sequence in the training set. When we plug this sequence into a Transformer, we are going to actually simultaneously train it to make prediction at every one. Now, **in this sequence of 9 characters, they are actually 8 individual examples**, the way it is processed is as follows:\n",
        "- in the context of [`18`], → `47` comes next\n",
        "- in the context of [`18, 47`], → `56` comes next\n",
        "- in the context of [`18,47,56`], → `58` comes next\n",
        "- ... and so on ...\n",
        "\n",
        "Let's spilled it out with code, see below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfjPExy-2UbW",
        "outputId": "0d67733e-bdc1-476c-d71b-92ad2d788e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target is 47\n",
            "when input is tensor([18, 47]) the target is 56\n",
            "when input is tensor([18, 47, 56]) the target is 57\n",
            "when input is tensor([18, 47, 56, 57]) the target is 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
          ]
        }
      ],
      "source": [
        "# Predicting the next token based on a given context\n",
        "\n",
        "# the first block of tokens\n",
        "x = train_data[:block_size] # x inputs to the Transformer, e.g. [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "\n",
        "# individual tokens shifted by one (also including the very last token now)\n",
        "y = train_data[1:block_size+1] # y is the target, # e.g. [2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "# iterating over the block size of 8\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target is {target}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeKJQ7Tj7be7"
      },
      "source": [
        "NB: this way of processing makes the Transformer network used to seeing **contexts** all the way from as little as one, all the way to the block size. See code explained below:\n",
        "\n",
        "**Data preparation**:\n",
        "- `x` is defined as the first block_size tokens from `train_data`, serving as the initial context or input sequence for the model.\n",
        "- `y` is essentially `x` shifted by one position to the right, indicating the next token that should be predicted by the model given the context in `x`. The last token in `y` goes beyond the initial `block_size` tokens to include the immediate next token in the sequence.\n",
        "\n",
        "**Iterative Prediction Task Setup**:\n",
        "- The loop `for t in range(block_size)`: iterates through each position in the block_size, creating increasingly larger contexts and their corresponding targets.\n",
        "- `context = x[:t+1]` gradually increases the context window by including one more token from x in each iteration. Initially, the context contains just the first token, and by the end of the loop, it includes all block_size tokens.\n",
        "- `target = y[t]` identifies the next token that the model should predict based on the given context. It's the token immediately following the last token in the current context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdDnMNyk_lzg"
      },
      "source": [
        "### Batch Size\n",
        "Extracting a batch of sequences and the corresponding next elements as targets from these datasets.\n",
        "\n",
        "**The batch size is the number of training examples processed before the model's internal parameters are updated**. For example, if you have a dataset of 1000 sentences and you choose a batch size of 100, the dataset will be divided into 10 batches. Each batch of 100 sentences will be passed through the network in sequence, with each pass followed by an update to the model's weights.\n",
        "\n",
        "**Impact on Training:**\n",
        "\n",
        "**Memory Usage**: A larger batch size requires more memory, as more data needs to be loaded and processed simultaneously. This can be a limiting factor depending on the hardware being used for training.\n",
        "**Convergence**: The choice of batch size can affect how quickly and smoothly the model converges to a solution. **Smaller batches** often lead to faster convergence but can result in a more erratic learning process. **Larger batches** provide more stable and accurate estimates of the gradient, but they might make the learning process slower and potentially get stuck in local minima.\n",
        "> **Generalization**: Some studies suggest that smaller batch sizes may lead to better generalization in the trained model. This is thought to be because the noise introduced by the smaller subsets helps to regularize the model.\n",
        "\n",
        "**Types of Batch Size**:\n",
        "\n",
        "- **Mini-Batch Gradient Descent**: This is the most common training method, where the batch size is a compromise between the extremes of 1 example per batch (stochastic gradient descent) and the entire dataset per batch (batch gradient descent). It balances the need for computational efficiency with the benefits of stochastic updates.\n",
        "- **Choosing Batch Size**: The optimal batch size is often determined experimentally, as it can depend on the specific task, the model architecture, and the hardware capabilities. Researchers and practitioners might start with a value that fits their system’s memory constraints and adjust based on training speed and model performance outcomes.\n",
        "\n",
        "In summary, batch size in NLP tasks (and machine learning more broadly) is a critical hyperparameter that influences the efficiency, convergence speed, and generalization performance of the training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PJ3T9qSFSTH"
      },
      "source": [
        "## Dataloader\n",
        "\n",
        "Every time we are going to feed inputs to the transformer, we are going to have **many batches of multiple chunks of text that are stacked up in a single tensor**. It is done for efficiency as GPUs are very good at the parallel processing of data. The 1-dimensional arrays are going to be stacked up to form a 4×8 tensor, that is with a sequence length (also called block size or context) of 4 and a batch size of 8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT6Hfcf6Ah94",
        "outputId": "9d9fd570-d7da-47ac-cba1-b090aa833f8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([8, 4])\n",
            "tensor([[ 1, 60, 39, 47],\n",
            "        [46, 43, 39, 60],\n",
            "        [ 1, 46, 43, 56],\n",
            "        [61, 47, 50, 50],\n",
            "        [43,  1, 39, 52],\n",
            "        [53, 58, 46,  1],\n",
            "        [53,  1, 40, 43],\n",
            "        [ 1, 56, 43, 45]])\n",
            "targets:\n",
            "torch.Size([8, 4])\n",
            "tensor([[60, 39, 47, 50],\n",
            "        [43, 39, 60, 43],\n",
            "        [46, 43, 56, 43],\n",
            "        [47, 50, 50,  1],\n",
            "        [ 1, 39, 52,  1],\n",
            "        [58, 46,  1, 40],\n",
            "        [ 1, 40, 43,  1],\n",
            "        [56, 43, 45, 39]])\n",
            "------\n",
            "when input is [1] the target is 60\n",
            "when input is [1, 60] the target is 39\n",
            "when input is [1, 60, 39] the target is 47\n",
            "when input is [1, 60, 39, 47] the target is 50\n",
            "when input is [46] the target is 43\n",
            "when input is [46, 43] the target is 39\n",
            "when input is [46, 43, 39] the target is 60\n",
            "when input is [46, 43, 39, 60] the target is 43\n",
            "when input is [1] the target is 46\n",
            "when input is [1, 46] the target is 43\n",
            "when input is [1, 46, 43] the target is 56\n",
            "when input is [1, 46, 43, 56] the target is 43\n",
            "when input is [61] the target is 47\n",
            "when input is [61, 47] the target is 50\n",
            "when input is [61, 47, 50] the target is 50\n",
            "when input is [61, 47, 50, 50] the target is 1\n",
            "when input is [43] the target is 1\n",
            "when input is [43, 1] the target is 39\n",
            "when input is [43, 1, 39] the target is 52\n",
            "when input is [43, 1, 39, 52] the target is 1\n",
            "when input is [53] the target is 58\n",
            "when input is [53, 58] the target is 46\n",
            "when input is [53, 58, 46] the target is 1\n",
            "when input is [53, 58, 46, 1] the target is 40\n",
            "when input is [53] the target is 1\n",
            "when input is [53, 1] the target is 40\n",
            "when input is [53, 1, 40] the target is 43\n",
            "when input is [53, 1, 40, 43] the target is 1\n",
            "when input is [1] the target is 56\n",
            "when input is [1, 56] the target is 43\n",
            "when input is [1, 56, 43] the target is 45\n",
            "when input is [1, 56, 43, 45] the target is 39\n"
          ]
        }
      ],
      "source": [
        "# Generating batches of sequences for training a model.\n",
        "\n",
        "torch.manual_seed(1337) # set the random number generator seed to a fixed value, i.e. 1337; important for reproducibility\n",
        "batch_size = 8  # number of sequences in a batch / processed in parallel\n",
        "block_size = 4  # maximum sequence length serving as a context/prompt\n",
        "\n",
        "def get_batch(split):\n",
        "    # Generate a batch of inputs/prompts x and respective targets y\n",
        "    # batches are always of shape (batch_size, block_size)\n",
        "    data = train_data if split == 'train' else val_data\n",
        "\n",
        "    # creating an index ix which will randomly select starting positions for sequences within the dataset.\n",
        "    # ix shape is equal to the batch size (8)\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "    # Accumulate and add each sequence of this batch to form a tensor\n",
        "    # tensor shape: batch size, block size\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "\n",
        "    # Same as x but shifted by one token\n",
        "    # tensor shape: batch size, block size\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # targets\n",
        "    return x, y # x is (4,8), y is (4,8) too\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('------')\n",
        "\n",
        "for b in range(batch_size):      # batch dimension, number of sequences in the batch (batch_size)\n",
        "    for t in range(block_size):  # time dimension, number of tokens in the sequence  (block_size)\n",
        "        context = xb[b, :t+1]    # context means prompt, taking the first t+1 tokens from the b-th sequence in the batch\n",
        "        target = yb[b, t]        # we take the t-th token from the b-th sequence in the batch for the target (the token we want to predict)\n",
        "        print(f\"when input is {context.tolist()} the target is {target}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "e51e5242-0c77-43da-d3d4-b80d27142b78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1, 60, 39, 47],\n",
            "        [46, 43, 39, 60],\n",
            "        [ 1, 46, 43, 56],\n",
            "        [61, 47, 50, 50],\n",
            "        [43,  1, 39, 52],\n",
            "        [53, 58, 46,  1],\n",
            "        [53,  1, 40, 43],\n",
            "        [ 1, 56, 43, 45]])\n"
          ]
        }
      ],
      "source": [
        "# this is our batch of inputs to feed to the transformer (tensor shape: (B,T) = batch size 8, block size 4)\n",
        "print(xb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bFOZqE0UW0-",
        "outputId": "7b17fd18-3d4c-4082-cf85-ce2b164b1ab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1, 60, 39, 47])\n"
          ]
        }
      ],
      "source": [
        "# amr\n",
        "# checking one row from the batch input\n",
        "print(xb[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPzXpwRNJQsJ"
      },
      "source": [
        "> NB: function `get_batch` code explained below:\n",
        "\n",
        "`ix = torch.randint(len(data) - block_size, (batch_size,))`\n",
        "This line generates a **tensor of random integers** `ix` using PyTorch's randint function. The integers are in the range [0, len(data) - block_size), which means **starting from 0 up to the length of the selected data minus block_size**. The size of the tensor is determined by batch_size, which means it will contain batch_size random integers. **These integers are used as the starting indices for the batches of data to be extracted**.\n",
        "\n",
        "`x = torch.stack([data[i:i+block_size] for i in ix])`\n",
        "This line uses a list comprehension to iterate over each starting index in ix and slices the data tensor from that index i to i + block_size, creating a sequence of data. `torch.stack` then combines these sequences into a new tensor x, where each sequence is a separate element in the batch. **This tensor x represents the input data for the model**.\n",
        "\n",
        "`y = torch.stack([data[i+1:i+block_size+1] for i in ix])`\n",
        "This line is similar to the previous one but creates the target tensor y. For each starting index i in ix, it slices the data tensor from i + 1 to i + block_size + 1. **This represents the target or \"next\" elements corresponding to the inputs in x**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alr2NLZf-lqk",
        "outputId": "5a466050-2679-4057-d06c-c819e3d3477f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ix: = {tensor([400784, 110140, 944762, 354070, 724297, 412236, 176790, 256488])}\n",
            "ix shape: = {torch.Size([8])}\n",
            "---\n",
            "ix: 400784\n",
            "x: tensor([56, 57, 46,  1])\n",
            "ix: 110140\n",
            "x: tensor([46, 53, 59,  1])\n",
            "ix: 944762\n",
            "x: tensor([53, 51, 44, 53])\n",
            "ix: 354070\n",
            "x: tensor([43, 56,  5, 57])\n",
            "ix: 724297\n",
            "x: tensor([58, 10,  1, 58])\n",
            "ix: 412236\n",
            "x: tensor([47, 52, 45,  1])\n",
            "ix: 176790\n",
            "x: tensor([51, 54, 50, 39])\n",
            "ix: 256488\n",
            "x: tensor([ 1, 53, 44,  1])\n",
            "x shape: torch.Size([8, 4])\n"
          ]
        }
      ],
      "source": [
        "# amr ******     testing the code above *******\n",
        "# generating a tensor with random sequence (tensor shape: batch size)\n",
        "ix = torch.randint(len(train_data) - block_size, (batch_size,))\n",
        "print(f\"ix: =\", {ix})\n",
        "print(f\"ix shape: =\", {ix.shape})\n",
        "print(\"---\")\n",
        "# amr - test iterating over the index ix and the input data i\n",
        "# stacking the sequences (tensor shape: batch size, block size)\n",
        "x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "for i in range(len(ix)):\n",
        "    print(f\"ix: {ix[i]}\")\n",
        "    print(f\"x: {x[i]}\")\n",
        "print(f\"x shape: {x.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: The above code generates batches of sequences for training a model. It randomly selects starting points within a dataset (train_data) and constructs sequences of fixed length (block_size) starting from these points. The resulting sequences are stacked together to form batches. This process is crucial for training models, especially for tasks like language modeling"
      ],
      "metadata": {
        "id": "z8NWP3-Hwmr_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKj9t31yNAey"
      },
      "source": [
        "## Implementing a simple bi-gram model\n",
        "A bi-gram model, in NLP, is a type of statistical language model that predicts the probability of a word given the preceding word. **It's called a bi-gram because it considers a \"gram\" (or token) in the context of one preceding gram, thus forming pairs or \"bi\"-grams**.\n",
        "\n",
        "**Example**: If your corpus had the sentence \"*The quick brown fox jumps*\", the bi-grams would be: \"The quick\", \"quick brown\", \"brown fox\", and \"fox jumps\". A bi-gram model would use these to calculate the likelihood of \"brown\" following \"quick\", \"fox\" following \"brown\", and so forth.\n",
        "\n",
        "In summary, a bi-gram model is a simple language model that can predict the next unit in a sequence based on the preceding one and is often used in applications requiring a balance between contextual relevance and computational simplicity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "2266dbce-ee40-4fb1-bd37-f089dcb0206d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logit shape {torch.Size([32, 65])}\n",
            "loss  {tensor(4.7724, grad_fn=<NllLossBackward0>)}\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ],
      "source": [
        "# This is a model that predicts the next token based on the previous token:\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # Create an embedding layer (vocab_size, embedding_dim)\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # 65 unique tokens in our \"vocabulary\", and for each token we have a 65 dimensional vector\n",
        "\n",
        "    def forward(self, idx, targets=None):       # if targets not provided, the method computes no loss\n",
        "        # idx and targets are both (B,T) tensor of integers (batch_size, block_size)\n",
        "        # it embeds the input indices using the token_embedding_table, resulting in logits\n",
        "        logits = self.token_embedding_table(idx) # B,T,C (4,8,65)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape        # B = batch_size, T = block_size, C = vocab_size\n",
        "            logits = logits.view(B*T, C)  # Transpose logits to (B*T, C)\n",
        "            # This is the first time we actively use the targets: containing the next token\n",
        "            targets = targets.view(B*T)   # Transpose targets to (B*T) (targets contains the next token's index for each input sequence in the batch)\n",
        "            # The loss is computed across all tokens in the batch based on the targets.\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "      # Generates new tokens based on the current context represented by input indices (idx) and\n",
        "      # a specified number of maximum new tokens to generate (max_new_tokens).\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "# Instantiate the model\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)              # Forward pass (yb remains unused for now)\n",
        "print(f\"logit shape\", {logits.shape}) # [B,T], vocabulary_size)\n",
        "print(f\"loss \", {loss})          # loss value\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgoFEaie91vn"
      },
      "source": [
        "NB: the text generation is garbage! because it is a totally random model, the token are not interconnected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVN1E6aiJg2c"
      },
      "source": [
        "**Every integer of our tokenized text is now represented by an embedding vector of size `vocab_size`**.<br>\n",
        "We do this by using **an embedding layer**. **This layer is effectively a lookup table that maps<br>**\n",
        "each possible (`vocab_size` are possible in total) character-representing index to a unique vector of size `vocab_size`.\n",
        "\n",
        "**The `logits` are the outputs of the model.<br>**\n",
        "We just treat the embedded tokens of the input batch as the logits.<br>\n",
        "This `logits` tensor holds all the embedded identities of the tokens in the input batch -> ($batch\\_size \\times block\\_size \\times vocab\\_size$).\n",
        "\n",
        "> We are *not yet* interconnecting the tokens with any sort of model/logic.<br>\n",
        "We are not yet training or predicting *anything*.\n",
        "\n",
        "This is about to change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paA-6Qm9NHwX"
      },
      "source": [
        "## Embedding Layer\n",
        "\n",
        "An input batch consists of tensors `xb` and `yb`.<br>\n",
        "Both `xb` and `yb` are of size $batch\\_size \\times block\\_size$.\n",
        "\n",
        "The batch is used as basis for 'sub-batching'.\n",
        "\n",
        "Because `yb` is just `xb` shifted by one token, we can use `yb` to train<br>\n",
        "on multiple examples *within a batches' partial sequences*, each being of different context size.\n",
        "\n",
        "These 'sub-batches' are called `context` and `target`. They are the pairs that we will feed into the model.\n",
        "\n",
        "For now, we can start focusing on the model itself and feed `xb` and later on `yb` into it.<br>\n",
        "\n",
        "We'll start building a bigram model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDOtDRjtW3LL",
        "outputId": "914ca9a4-ac10-4bcc-bf0a-9b822d672fe4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BigramLanguageModel(\n",
              "  (token_embedding_table): Embedding(65, 65)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# amr\n",
        "# looking at the model\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMo_g18IFvR5",
        "outputId": "969db6e5-848d-4d72-f153-27b3aa8c3b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedding table shape: \n",
            " torch.Size([65, 65])\n",
            "Embeddings weights: \n",
            " tensor([[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
            "        [ 1.3035, -0.4501,  1.3471,  ...,  0.1910, -0.3425,  1.7955],\n",
            "        ...,\n",
            "        [ 0.4222, -1.8111, -1.0118,  ...,  0.5462,  0.2788,  0.7280],\n",
            "        [-0.8109,  0.2410, -0.1139,  ...,  1.4509,  0.1836,  0.3064],\n",
            "        [-1.4322, -0.2810, -2.2789,  ..., -0.5551,  1.0666,  0.5364]])\n"
          ]
        }
      ],
      "source": [
        "# amr\n",
        "# Access the embeddings weight\n",
        "embeddings_weight = m.token_embedding_table.weight.data\n",
        "\n",
        "print(\"embedding table shape: \\n\", embeddings_weight.shape)\n",
        "print(\"Embeddings weights: \\n\", embeddings_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dE8lj4nLGO5O",
        "outputId": "7a5a7d56-956d-4bc4-ba1c-f74f0455e1ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings for token at index 0 : tensor([ 0.1808, -0.0700, -0.3596, -0.9152,  0.6258,  0.0255,  0.9545,  0.0643,\n",
            "         0.3612,  1.1679, -1.3499, -0.5102,  0.2360, -0.2398, -0.9211,  1.5433,\n",
            "         1.3488, -0.1396,  0.2858,  0.9651, -2.0371,  0.4931,  1.4870,  0.5910,\n",
            "         0.1260, -1.5627, -1.1601, -0.3348,  0.4478, -0.8016,  1.5236,  2.5086,\n",
            "        -0.6631, -0.2513,  1.0101,  0.1215,  0.1584,  1.1340, -1.1539, -0.2984,\n",
            "        -0.5075, -0.9239,  0.5467, -1.4948, -1.2057,  0.5718, -0.5974, -0.6937,\n",
            "         1.6455, -0.8030,  1.3514, -0.2759, -1.5108,  2.1048,  2.7630, -1.7465,\n",
            "         1.4516, -1.5103,  0.8212, -0.2115,  0.7789,  1.5333,  1.6097, -0.4032,\n",
            "        -0.8345])\n"
          ]
        }
      ],
      "source": [
        "# amr\n",
        "# Token index to check\n",
        "token_index = 0  # Replace with the index of the token you want to check\n",
        "\n",
        "# Get the embeddings for the token at the specified index\n",
        "token_embeddings = embeddings_weight[token_index]\n",
        "\n",
        "print(\"Embeddings for token at index\", token_index, \":\", token_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qR8NgBXcg2d"
      },
      "source": [
        "> NB: Code Explaination for the above forward pass and text generation:\n",
        "\n",
        "**Forward Pass**: `forward(self, idx, targets=None)`\n",
        "\n",
        "**Parameters**:\n",
        "\n",
        "- `idx`: A tensor of shape `(B, T)` containing indices of tokens, where `B` is the batch size and `T` is the sequence length.\n",
        "- `targets`: A tensor of shape `(B, T)` containing the indices of the target tokens. If targets is not provided, the method computes no loss.\n",
        "\n",
        "**Process**:\n",
        "\n",
        "- The method retrieves the logits for each token in `idx` using the token_embedding_table.\n",
        "- If `targets` is provided, it reshapes the logits to `(B*T, C)` and the targets to `(B*T)`, then computes the cross-entropy loss between the logits and the targets. This loss measures how well the model predicts the next token.\n",
        "- Returns: The logits and the computed loss (if `targets` is provided; otherwise, `loss` is `None`).\n",
        "\n",
        "**Text Generation**: `generate(self, idx, max_new_tokens)`\n",
        "\n",
        "**Parameters**:\n",
        "\n",
        "- `idx`: A tensor of shape `(B, T)` representing the initial context for generation.\n",
        "- `max_new_tokens`: The maximum number of new tokens to generate.\n",
        "\n",
        "**Process**:\n",
        "\n",
        "- The method iteratively generates one token at a time based on the current context (idx).\n",
        "- It updates the context by appending the newly generated token and repeats the process until `max_new_tokens` have been generated.\n",
        "- For each new token, it computes the logits for the last token in the current context, applies softmax to convert these logits into probabilities, and samples a new token index from this probability distribution.\n",
        "- Returns: The tensor `idx` containing the original context plus the newly generated tokens.\n",
        "\n",
        "**Model Instantiation and Usage**\n",
        "\n",
        "- An instance of `BigramLanguageModel` is created with a specified vocab_size.\n",
        "- The model is then used to compute logits and loss for a given batch of inputs `(xb, yb)` and to generate text starting from an initial context.\n",
        "\n",
        "**Function Calls and Outputs**\n",
        "\n",
        "- `logits.shape` and `loss`: Prints the shape of the logits tensor and the value of the loss computed during the forward pass.\n",
        "- decode(...): Assuming the decode function maps token indices back to their string representations, this line generates 100 new tokens starting from an initial context of a single zero index and prints the decoded text.\n",
        "\n",
        "This model is a simplistic representation of language modeling and is more illustrative than practical, especially since the embedding table is used in an unconventional way to directly produce logits for next-token prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT3L59FztojH"
      },
      "source": [
        "## Setting up the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD8QpJKPtlg2"
      },
      "source": [
        "## Basic Pytorch Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96G0TS6YY0hG",
        "outputId": "263732f7-7b84-416f-cc6c-0f9c1c9c93c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.841165542602539\n"
          ]
        }
      ],
      "source": [
        "# basic Pytorch training loop\n",
        "batch_size = 32\n",
        "for steps in range(1000): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    # The model m processes the batch xb and compares the predictions (logits) against the actual targets yb to compute the loss,\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True) # argument is an optimization that can potentially speed up gradient zeroing.\n",
        "    loss.backward()                       # Computes the gradient of the loss\n",
        "    optimizer.step()                      # Updates the model parameters based on the computed gradients and the optimizer algorithm\n",
        "\n",
        "print(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "da98f5c3-01ea-4708-b4d1-10a552337091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "olylvLLko'TMya!PToconxad.?-tNSqYPsx&bF.oiR;BD$dZBMZv'K f bRSmIKptRPly:AUC&$zLK,qUEy&Ay;ZxjKVhmrdagC-bTop$EJe.H?x\n",
            "JGF&pUct-P  ti.hlEsu;w:w a BG:tLhMk,epdhlay'sVzLq-3ERwXUzDnq-bn czXxxI&V&Pynnl,s,Ioto!uvixwC-IJXElrgm C-.bcoCPJ\n",
            "prI3fa kO AL!ZvWAIkpr.SesPHEJUzV;P?uN3b?ohoRiBUENfy3B&jumNL;Aik,\n",
            "xf -IEKROnasfyYWW?nv'ay;:weOlgqVzPyoiBL? sEwI3DVr,iy.xyIcf r!!ul-Koi:x pZrAQly'v'a;vEzN\n",
            "BwowKo'RMqF$PPFb\n",
            "CjYX3beT,lZ qdda!wfgmJP\n",
            "DUfNXmnQU mvcv?nlnQF$Jz:AywNocd  bGSPyAlprNeQnq-GRSVUP.Ja!IBoDqfI&xJM AXEHV&DKvRS\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeZ3eZL8agqy"
      },
      "source": [
        "NB: the output is much better! more like Shakespeare writings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JU8omhOuD1W"
      },
      "source": [
        "Given that we have the identities of the next character through `yb`, how well does the model predict them through the `logits`? **The `loss` is the measurement of prediction quality**.\n",
        "\n",
        "We want the index within `yb` to be the same as the most likely/active index within `logits`.<br>\n",
        "The loss is measured as the average of this across all the tokens in the input batch.\n",
        "\n",
        "We know the `vocab_size` is $65$.<br>\n",
        "We can calculate what the loss should be if we were to predict the next token totally randomly:\n",
        "\n",
        "$$-ln(\\frac{1}{65}) = 4.1743872699$$\n",
        "\n",
        "**Our calculated loss is higher/worse, because we are not predicting perfectly randomly to begin with.<br>**\n",
        "**The initial predictions are not perfectly spread out across the `vocab_size`.<br>**\n",
        "**They aren't super diffuse and contain a bit of entropy.<br>**\n",
        "**We haven't yet learned uniform distribution across the `vocab_size`**.\n",
        "\n",
        "![](https://images.squarespace-cdn.com/content/56316c94e4b098620a45e78a/1457973972468-D5XJVA1ABFXSD0AH9RZC/?content-type=image%2Fpng)\n",
        "<br>Source: [Shiken](https://shiken.ai/chemistry/entropy)\n",
        "\n",
        "The `loss` is to be minimized.<br>\n",
        "We will need the model to make predictions of individual next tokens.<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZ68oSrDo_Jo",
        "outputId": "e771dd1b-7185-492c-c90c-cb59de8803f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# amr ***** test section ******\n",
        "# the code below at token 0 will give a line return\n",
        "idx = torch.zeros(1,1); idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hrJaz_noQoy"
      },
      "source": [
        "> NB: code explained for the text generation, see below:\n",
        "\n",
        "Generation setup\n",
        "\n",
        "- `m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)`: This line generates a sequence of 500 new tokens starting from an initial context provided by `idx`. **Here, `idx` is initialized as a tensor of zeros with shape (1, 1), implying that the generation starts with no specific context, actually token 0 will produce a line return**. The model `m` likely uses a form of prediction mechanism (such as sampling or greedy selection) based on the learned distributions to choose the next token at each step.\n",
        "\n",
        "Token list conversion\n",
        "\n",
        "- `[0].tolist()`: The generated sequence is accessed (assuming the model may return a batch of sequences, and we're interested in the first) and converted to a Python list of token indices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAKfOKUiJ-r3"
      },
      "source": [
        "## The mathematical trick in self-attention\n",
        "- matrix multiplication\n",
        "- applying a mask\n",
        "- attention mechanism\n",
        "\n",
        "Let's go through different toy examples below."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrix Multiplication - Matmul\n",
        "Matrix multiplication is like combining two tables of numbers to create a new table. You take each row from the first table and multiply it by each column from the second table, adding up the results to fill in the new table. The size of the resulting table depends on the dimensions of the original tables.\n",
        "\n",
        "[matrix multiplication](http://matrixmultiplication.xyz/)"
      ],
      "metadata": {
        "id": "4so1ty0tnDY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example on matrix multiplication\n",
        "torch.manual_seed(42)\n",
        "a = torch.ones(3,3)\n",
        "b = torch.randint(0,10, (3,2)).float()\n",
        "c = a@b\n",
        "print(\"a=\")\n",
        "print(a)\n",
        "print(\"---\")\n",
        "print(\"b=\")\n",
        "print(b)\n",
        "print(\"---\")\n",
        "print(\"c= a@b\")\n",
        "print(c)\n",
        "print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRENozI1sFjx",
        "outputId": "8025e6b8-5c33-455d-db3a-a2a346172c24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "---\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "---\n",
            "c= a@b\n",
            "tensor([[14., 16.],\n",
            "        [14., 16.],\n",
            "        [14., 16.]])\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Operations on Matrices"
      ],
      "metadata": {
        "id": "hLUVF-OtpBbC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTSzuZvngQtF",
        "outputId": "d779b3b9-fddd-40c1-c143-fd34efb7b955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix a orig = \n",
            "tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]])\n",
            "--\n",
            "Matrix a normalized = \n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "Matrix b  = \n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "Matrix c = matmul a@b\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n",
            "--\n"
          ]
        }
      ],
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "\n",
        "# This sets the seed for the random number generator for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# tensor a (3x3) - triangular matrix\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "print(\"Matrix a orig = \")\n",
        "print(a)\n",
        "print('--')\n",
        "\n",
        "# tensor a: normalizing  each row of the tensor by dividing it by the sum of its elements,\n",
        "#   computes the sum of elements along the columns (dimension 1), and\n",
        "#   keepdim=True keeps the dimension for broadcasting to work correctly during the division.\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "print(\"Matrix a normalized = \")\n",
        "print(a)\n",
        "print('--')\n",
        "\n",
        "# tensor b (2 x 3) of random integers between 0 and 9, from a uniform distribution\n",
        "# The .float() method converts the tensor from an integer type to a floating-point type, necessary for subsequent matrix operations\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "print(\"Matrix b  = \")\n",
        "print(b)\n",
        "print('--')\n",
        "\n",
        "# Pytorch matrix multiplication\n",
        "c = a @ b\n",
        "print(\"Matrix c = matmul a@b\")\n",
        "print(c)\n",
        "print('--')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhIFsP3Bj4tf"
      },
      "source": [
        "NB: matrices summary ;\n",
        "- Matrix a: A 3x3 lower triangular matrix with normalized rows.\n",
        "- Matrix b: A 3x2 matrix of random floating-point numbers between 0 and 9.\n",
        "- Matrix c: The result of the matrix multiplication of a with b, a 3x2 matrix.\n",
        "\n",
        "\n",
        "> **Matrix c**:\n",
        "- The first row of c is identical to the first row of b because the first row of a is [1, 0, 0], meaning it multiplies the first element of b by 1 and adds zeros for the rest.\n",
        "- The second row of c is the average of the first and second rows of b because the second row of a is [0.5, 0.5, 0].\n",
        "- The third row of c is the average of all three rows of b because the third row of a is [0.3333, 0.3333, 0.3333]."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input tensor B,T,C\n",
        "- B = Batch = 4\n",
        "- T = Time or sequence length = 8\n",
        "- C = channels or vocab size = 2\n",
        "\n",
        "So, our input has a batch of 4 sequences, each sequence of length is 8, and each element of the sequence has 2 channels (features)."
      ],
      "metadata": {
        "id": "Em9azAx9vUM7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfLNmQgRJ-r4",
        "outputId": "f360cbed-f773-47e6-89de-2992e0aea2f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input tensor x = \n",
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.3596, -0.9152],\n",
            "         [ 0.6258,  0.0255],\n",
            "         [ 0.9545,  0.0643],\n",
            "         [ 0.3612,  1.1679],\n",
            "         [-1.3499, -0.5102],\n",
            "         [ 0.2360, -0.2398],\n",
            "         [-0.9211,  1.5433]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.2858,  0.9651],\n",
            "         [-2.0371,  0.4931],\n",
            "         [ 1.4870,  0.5910],\n",
            "         [ 0.1260, -1.5627],\n",
            "         [-1.1601, -0.3348],\n",
            "         [ 0.4478, -0.8016],\n",
            "         [ 1.5236,  2.5086]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 1.0101,  0.1215],\n",
            "         [ 0.1584,  1.1340],\n",
            "         [-1.1539, -0.2984],\n",
            "         [-0.5075, -0.9239],\n",
            "         [ 0.5467, -1.4948],\n",
            "         [-1.2057,  0.5718],\n",
            "         [-0.5974, -0.6937]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.3514, -0.2759],\n",
            "         [-1.5108,  2.1048],\n",
            "         [ 2.7630, -1.7465],\n",
            "         [ 1.4516, -1.5103],\n",
            "         [ 0.8212, -0.2115],\n",
            "         [ 0.7789,  1.5333],\n",
            "         [ 1.6097, -0.4032]]])\n",
            "--\n",
            "tensor x shape \n",
            " {torch.Size([4, 8, 2])}\n"
          ]
        }
      ],
      "source": [
        "# consider the following toy example:\n",
        "torch.manual_seed(1337) # set the seed for reproducibility\n",
        "B,T,C = 4,8,2           # batch, time, channels\n",
        "x = torch.randn(B,T,C)  # random numbers for a tensor of shape B,T,C\n",
        "print(\"input tensor x = \")\n",
        "print(x)\n",
        "print('--')\n",
        "print(f\"tensor x shape \\n\", {x.shape})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O9IO2LgpZ1g"
      },
      "source": [
        "NB: `x` can represent an input tensor to a neural network where you have a **batch of 4 sequences, each sequence of length 8, and each element of the sequence has 2 channels** (features)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jqhq4-kJp5_h"
      },
      "source": [
        ">- So, here we have  8  tokens, each of which is a vector of size  2 .\n",
        "- They are not talking to each other / are not related to each other in any way.\n",
        "- We'd like to couple them so that e.g. the 3rd token can only communicate with the tokens in the 2nd and 1st location, but not with a future token in the 4th location.\n",
        "- Information has to be able to flow, but exclusively in one direction.\n",
        "- We can do this in a most simple way by averaging preceding tokens, including the current_token. This would, in essence, summarize current_token in the context of current_token's history.\n",
        "- For every  𝑡 -th token, we'd like to get the average of all the vectors of previous tokens and the current one ( 𝑡 ) as well:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing a running average - A For loop"
      ],
      "metadata": {
        "id": "Ncxi2_MhoGYN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_9pvaV5ZJAR",
        "outputId": "67b8e918-d1cc-460f-bed7-2ce8a6e6b4e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch [0]:\n",
            " tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398],\n",
            "        [-0.9211,  1.5433]]) \n",
            "\n",
            "Running Averages:\n",
            " tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]])\n"
          ]
        }
      ],
      "source": [
        "# We want x[b, t] = mean_{i <= t} x[b, i]\n",
        "xbow = torch.zeros((B, T, C))          # Create tensor of zeros of shape (B, T, C) (bag of words representation of the input)\n",
        "for b in range(B):                     # For all batches\n",
        "    for t in range(T):                 # For all tokens in the batch\n",
        "        xprev = x[b, :t+1]             # Get all tokens up to and including the current token (t, C)\n",
        "        xbow[b, t] = xprev.mean(dim=0) # Calculate the mean of the tokens up to and including the current token\n",
        "\n",
        "print('Batch [0]:\\n', x[0], \"\\n\")     # First batch of 8 tokens, each of size 2\n",
        "print('Running Averages:\\n', xbow[0]) # Running averages of the first batch of 8 tokens, each of size 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAXaGKJOZJAR"
      },
      "source": [
        "NB: Due to the loops, this is relatively inefficient.<br>**The trick is that we can build a running average like this using<br>\n",
        "much faster matrix multiplication:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing a running average - Matrix Multiplication"
      ],
      "metadata": {
        "id": "mzHliib5n0i5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzmEqXTIKbtz",
        "outputId": "ac23e43d-5145-4ba0-f419-09a1fae4b151"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch [0]:\n",
            " tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398],\n",
            "        [-0.9211,  1.5433]]) \n",
            "\n",
            "Running Averages:\n",
            " tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]])\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# version 2: it uses a weighted sum approach, leveraging matrix multiplication for efficiency.\n",
        "wei = torch.tril(torch.ones(T, T))      # Lower triangular matrix of ones (tril used here)\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x                         # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "print('Batch [0]:\\n', x[0], \"\\n\")       # First batch of 8 tokens, each of size 2\n",
        "print('Running Averages:\\n', xbow2[0])  # Running averages of the first batch of 8 tokens, each of size 2\n",
        "\n",
        "# comparing the bag of words (xbow, xbow2)\n",
        "# Set a higher absolute tolerance\n",
        "atol_value = 1e-6  # For example, you can adjust this value as needed\n",
        "\n",
        "# Use the torch.allclose function with the custom atol\n",
        "close = torch.allclose(xbow, xbow2, atol=atol_value)\n",
        "print(close)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: Both approaches, the `for` loop and matmul (`@`) give the same results. The second one, computing via matmul is much more performant and is used in ML."
      ],
      "metadata": {
        "id": "KjsVkddOw-Kb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing the softmax"
      ],
      "metadata": {
        "id": "E64Pk15eormU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwSlBlSfJ-r4",
        "outputId": "a6c63372-594e-4220-d838-7dda78293598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "Batch [0]:\n",
            " tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398],\n",
            "        [-0.9211,  1.5433]]) \n",
            "\n",
            "Running Averages:\n",
            " tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]])\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# version 3: it uses the Softmax\n",
        "\n",
        "# a triangular matrix - 2D tensor with shape (T,T)\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "\n",
        "#  tensor initialized with zeros - tensor 8x8\n",
        "wei = torch.zeros((T,T))\n",
        "\n",
        "# tensor masked fill with 0 and -inf (negative infinity) wherever the corresponding element in tril is zero.\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "print(wei)\n",
        "\n",
        "# applying softmax - converts the weights into a probability distribution, effectively performing the softmax operation on each row of wei.\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "# matrix multiplication\n",
        "xbow3 = wei @ x\n",
        "\n",
        "print('Batch [0]:\\n', x[0], \"\\n\")       # First batch of 8 tokens, each of size 2\n",
        "print('Running Averages:\\n', xbow3[0])  # Running averages of the first batch of 8 tokens, each of size 2\n",
        "\n",
        "# comparing the bag of words (xbow, xbow2)\n",
        "# Set a higher absolute tolerance\n",
        "atol_value = 1e-6  # For example, you can adjust this value as needed\n",
        "\n",
        "# Use the torch.allclose function with the custom atol\n",
        "close = torch.allclose(xbow, xbow3, atol=atol_value)\n",
        "print(close)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: all the toy examples above `xbow`, `xbow2`, `xbow3` give the same result, we obtained a running averages."
      ],
      "metadata": {
        "id": "DSVbP3OtyM3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-Attention Mechanism\n",
        "\n",
        "The self-attention mechanism allows the model to weigh the importance of different parts of the input sequence when processing each element of the sequence. This is a key component in Transformer models and is crucial for tasks such as sequence modeling and language understanding.\n",
        "\n"
      ],
      "metadata": {
        "id": "w6sSibQpFtvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How self-attention works?\n",
        "\n",
        "Here's a step-by-step breakdown of how self-attention works:\n",
        "\n",
        "**Input Representation**: Each item in the input sequence is represented by a vector. These vectors serve as the queries (Q), keys (K), and values (V) in the self-attention mechanism.\n",
        "\n",
        "**Dot Products of Queries and Keys** (Q@K): For each item (query) in the input sequence, we compute the dot product with all keys. This operation produces a score that represents the similarity between each query and all keys. The idea is that higher scores will indicate higher relevance of other parts of the input to the current word.\n",
        "\n",
        "**Scaling**: The scores are scaled down by dividing them by the square root of the dimension of the key vectors. This scaling helps in stabilizing the gradients during training, as it prevents the dot products from growing too large in magnitude.\n",
        "\n",
        "**Softmax**: A softmax function is applied to the scaled scores. This operation converts the scores to probabilities, ensuring that they sum up to 1. The softmax operation thus allows the model to focus more on the items with higher scores, effectively determining the level of attention each item should get.\n",
        "\n",
        "**Multiply Scores with Values** (V): The softmax probabilities are then used to weight the values. The weighted sum of the values is computed, resulting in a new vector for each item (V). **This vector is a representation of the item, enriched with information from other relevant parts of the input sequence**.\n",
        "\n",
        "**Output**: The output is a sequence of these new vectors, one for each item in the input. This output can then be fed into subsequent layers of the Transformer model."
      ],
      "metadata": {
        "id": "AMqxX6ah-9d2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Self-Attention, every single token in the batch emits two vectors: **query** and **key**:\n",
        "- The **query vector** is the token-specific \"What am I looking for?\" information\n",
        "- The **key vector** is the token-specific \"What do I contain?\" information\n",
        "\n",
        "To establish **affinity** (high interrelation and high influence on the sampling decision) between tokens of the batch, we calculate the dot product of the query and key vectors of each token with each other token in the batch.\n",
        "**This is the affinity matrix or in our case wei, also called the attention weight**.\n",
        "\n",
        "If during dot product calculation the key and the query turn out to be well aligned or similar, the affinity will be high. If they are not, the affinity will be low.\n",
        "\n",
        "Let's build the individual head:"
      ],
      "metadata": {
        "id": "ldsR9fmk_ta4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a single self-attention head\n",
        "\n",
        "We need to do the following:\n",
        "\n",
        "To create a single attention head, you can follow these steps based on the information provided in the sources:\n",
        "\n",
        "1. **Initialize Parameters**: Each attention head has its own set of learnable weight matrices for queries, keys, and values.\n",
        "2. **Compute Attention Scores**: Multiply the queries, keys, and values matrices with their respective weight matrices for the single head.\n",
        "3. **Apply Attention Function**: Use a single attention function by multiplying the queries and keys matrices, applying scaling and softmax operations, and weighting the values matrix to generate an output for the single head.\n",
        "4. **Concatenate Outputs**: In the case of a single head, there is no need to concatenate outputs from multiple heads.\n",
        "5. **Linear Projection**: Apply a linear projection to the output of the single head using a weight matrix to generate the final result.\n",
        "\n",
        "See code below."
      ],
      "metadata": {
        "id": "AQItLCzW__pD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS0HxWZwJ-r5",
        "outputId": "8e5ed9fb-4748-453c-9268-38d4e4fd66b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei shape {torch.Size([4, 8, 8])}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337) # for reproducibility\n",
        "\n",
        "B,T,C = 4,8,32 # batch (4), time (or sequence length (8)), # of channels (32) (or features) for the input tensor\n",
        "\n",
        "# tensor x initialized with random values drawn from a normal distribution with mean 0 and variance 1.\n",
        "# Tensor shape (4, 8, 32)\n",
        "x = torch.randn(B,T,C)  # Random input of shape (B, T, C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "\n",
        "# Sets the size of each attention head.\n",
        "head_size = 16\n",
        "\n",
        "# Initializes three linear transformations without bias for the key, query, and value projections\n",
        "# in the attention mechanism. Each transformation projects the input tensor\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16) # Applies the key transformation to x, resulting in k of shape (B, T, 16).\n",
        "q = query(x) # (B, T, 16) # Applies the query transformation to x, resulting in q of shape (B, T, 16).\n",
        "\n",
        "# Performs matrix multiplication between q and the transpose of k along the last two dimensions,\n",
        "# resulting in a weight tensor wei of shape (B, T, T).\n",
        "# These weights represent the raw attention scores before normalization.\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "# Creates a lower-triangular matrix of ones (and zeros elsewhere) of size (T, T).\n",
        "# This will be used to mask future time steps in the attention scores,\n",
        "# enforcing causality (each position can only attend to positions before it).\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "\n",
        "# Creates a lower-triangular matrix of ones (and zeros elsewhere) of size (T, T).\n",
        "# This will be used to mask future time steps in the attention scores, enforcing causality\n",
        "# (each position can only attend to positions before it).\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))   # B,T,T\n",
        "print(f\"wei shape\", {wei.shape})\n",
        "\n",
        "# applying softmax to the masked attention scores along the last dimension,\n",
        "# converting them into proper probability distributions.\n",
        "# The softmax function effectively nullifies the positions set to negative infinity by converting them to zero.\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "# Applies the value transformation to x, resulting in v of shape (B, T, 16).\n",
        "v = value(x)\n",
        "\n",
        "# Multiplies the normalized attention weights wei with the value tensor v, resulting in the output tensor out of shape (B, T, 16).\n",
        "# This operation is the weighted sum step of the attention mechanism.\n",
        "out = wei @ v\n",
        "\n",
        "\n",
        "out.shape   # B, T, 16"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: Breakdown of the self-attention mechanism from example above.\n",
        "\n",
        "**Initialization**:\n",
        "\n",
        "The random seed is set for reproducibility.\n",
        "B, T, and C represent the batch size, sequence length, and number of channels (or features) for the input tensor, respectively.\n",
        "A random input tensor x of shape (B, T, C) is created with values drawn from a normal distribution.\n",
        "\n",
        "**Linear Transformations**:\n",
        "\n",
        "Three linear transformations (key, query, and value) without bias are initialized. Each transformation projects the input tensor x into a lower-dimensional space (head_size), resulting in tensors k, q, and v.\n",
        "Attention Scores:\n",
        "\n",
        "**Attention scores** wei are computed by performing a dot product between the query tensor q and the transpose of the key tensor k. This operation results in a tensor of shape (B, T, T), representing raw attention scores before normalization.\n",
        "\n",
        "**Masking**:\n",
        "\n",
        "A lower-triangular matrix of ones (and zeros elsewhere) of size (T, T) is created to mask future time steps in the attention scores, enforcing causality (each position can only attend to positions before it).\n",
        "The attention scores are masked by filling the positions corresponding to future time steps with negative infinity.\n",
        "\n",
        "**Normalization**:\n",
        "\n",
        "The masked attention scores are normalized using the softmax function along the last dimension, converting them into proper probability distributions.\n",
        "\n",
        "**Weighted Sum**:\n",
        "\n",
        "The normalized attention weights are multiplied with the value tensor v, resulting in the output tensor out. This step computes the weighted sum of the values based on the attention weights.\n",
        "\n",
        "**Output Shape**:\n",
        "\n",
        "The shape of the output tensor out is (B, T, head_size).\n",
        "This code snippet demonstrates the core components of a self-attention mechanism, including key, query, and value projections, attention score computation, masking, normalization, and weighted sum calculation."
      ],
      "metadata": {
        "id": "uFMBD6yDKm86"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq1XihWQJ-r5",
        "outputId": "f0cb42fd-1c61-4b6e-e2c2-b2fc0731c227"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# extracting one record weight\n",
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn4yHPkgJ-r5"
      },
      "source": [
        "Notes:\n",
        "> - Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "992e944d-8b71-4ae9-97a1-6ee2767356e8",
        "id": "cmGr_qn93tlq"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input tensor x \n",
            " {tensor([[[ 0.1808, -0.0700, -0.3596,  ..., -0.8016,  1.5236,  2.5086],\n",
            "         [-0.6631, -0.2513,  1.0101,  ...,  1.5333,  1.6097, -0.4032],\n",
            "         [-0.8345,  0.5978, -0.0514,  ..., -0.4370, -1.0012, -0.4094],\n",
            "         ...,\n",
            "         [-0.8961,  0.0662, -0.0563,  ...,  2.1382,  0.5114,  1.2191],\n",
            "         [ 0.1910, -0.3425,  1.7955,  ...,  0.3699, -0.5556, -0.3983],\n",
            "         [-0.5819, -0.2208,  0.0135,  ..., -1.9079, -0.5276,  1.0807]],\n",
            "\n",
            "        [[ 0.4562, -1.0917, -0.8207,  ...,  0.0512, -0.6576, -2.5729],\n",
            "         [ 0.0210,  1.0060, -1.2492,  ...,  0.7859, -1.1501,  1.3132],\n",
            "         [ 2.2007, -0.2195,  0.5427,  ..., -0.6445,  1.0834, -0.7995],\n",
            "         ...,\n",
            "         [ 0.3091,  1.1661, -2.1821,  ...,  0.6151,  0.6763,  0.6228],\n",
            "         [ 0.0943, -0.3156,  0.7850,  ..., -1.5735,  1.3876,  0.7251],\n",
            "         [ 0.6455, -0.3313, -1.0390,  ...,  0.0895, -0.3748, -0.4781]],\n",
            "\n",
            "        [[-0.6067,  1.8328,  0.2931,  ...,  1.0041,  0.8656,  0.1688],\n",
            "         [-0.2352, -0.2586,  0.0131,  ...,  0.6690,  0.7535, -0.5359],\n",
            "         [-1.0277,  0.5347, -0.7958,  ...,  1.0711,  0.4901, -0.4876],\n",
            "         ...,\n",
            "         [-0.6896, -0.7080, -0.3152,  ..., -2.0662, -1.1418, -0.1391],\n",
            "         [ 1.0827,  1.1522,  0.5198,  ...,  0.4970,  0.0585,  0.1033],\n",
            "         [ 0.0720,  1.1080,  0.7293,  ...,  0.3967, -0.9755,  0.5122]],\n",
            "\n",
            "        [[ 0.3330,  1.0995,  0.4034,  ...,  1.6634, -0.4718,  0.5857],\n",
            "         [-0.9579,  0.9435, -2.1992,  ..., -0.7296,  0.1653, -0.3390],\n",
            "         [ 1.5416,  1.0231,  1.3392,  ..., -0.0433, -0.2505, -0.7493],\n",
            "         ...,\n",
            "         [ 0.7450,  0.7170,  1.2668,  ...,  1.9359,  2.0350,  2.0187],\n",
            "         [ 0.0323, -0.6337,  0.2938,  ..., -0.3297, -0.0192,  0.9225],\n",
            "         [ 0.9187,  0.2998,  0.6106,  ...,  0.8282, -0.4826,  1.8330]]])}\n",
            "input tensor x shape \n",
            " {torch.Size([4, 8, 32])}\n",
            "---\n",
            "query \n",
            " {tensor([[-0.6567,  0.0283,  0.0094, -0.6995, -0.3604,  0.8376, -0.4446,  0.1228,\n",
            "          0.6276, -0.6222,  0.3483,  0.2411,  0.5409, -0.2605,  0.3612, -0.0436],\n",
            "        [-0.3932,  0.8220, -0.7027,  0.0954, -0.1222, -0.1518, -0.5024, -0.4636,\n",
            "          0.1176,  1.4282, -0.5812,  0.1401,  0.9604,  0.0410, -0.6214, -0.6347],\n",
            "        [ 0.2157, -0.3507,  0.0022,  0.4232, -0.2284, -0.0732, -0.3412,  0.9647,\n",
            "         -0.5178,  0.0921, -0.5043,  0.8388,  0.6149, -0.0109, -0.5569,  0.5820],\n",
            "        [ 0.9000, -0.1272,  0.5458,  0.4254, -0.4513, -0.0212,  0.1711,  0.2599,\n",
            "         -0.9978,  0.4890,  0.1737, -0.0700, -0.3113,  0.3748, -0.1848, -0.6379],\n",
            "        [ 0.0332,  0.5886, -0.4437,  0.3775, -0.6826, -0.2775,  0.4673, -1.2956,\n",
            "          0.6603,  0.1633, -1.7573, -0.6582, -0.2302, -0.0862, -0.0060,  0.7573],\n",
            "        [ 0.2098,  0.0439, -0.0702,  0.0727, -0.2012, -1.7539,  1.0369,  0.1163,\n",
            "          0.2956,  0.3231,  0.5052,  0.7011, -0.2844, -0.7844,  0.4782, -0.5170],\n",
            "        [ 0.6100, -0.3284, -0.8557,  0.8543,  0.7805, -0.4023, -0.8183, -0.0554,\n",
            "          0.1873,  0.2706, -0.7066, -0.8637,  0.6998, -0.0670,  0.2551,  0.2149],\n",
            "        [ 0.1459,  0.1349, -0.2335, -0.0417,  0.2928, -0.5080,  0.1177,  0.1861,\n",
            "          0.1455,  0.0292, -0.8470,  0.6116,  1.2445,  0.1909,  0.3694, -0.0027]],\n",
            "       grad_fn=<SelectBackward0>)}\n",
            "query shape \n",
            " {torch.Size([4, 8, 16])}\n",
            "---\n",
            "key \n",
            " {tensor([[ 0.1196, -0.3013,  0.3629,  1.1771,  1.1385, -0.2554,  0.1454, -0.2944,\n",
            "         -0.7020, -1.0308,  0.7436, -0.8098, -0.6669,  0.0912, -0.0061,  0.1983],\n",
            "        [-0.5423, -0.5558, -0.0761,  1.2929,  0.8653, -1.1998,  0.3878,  0.1939,\n",
            "          0.7024, -0.8225,  0.2348, -0.8499, -0.3813, -0.2991,  0.0102, -0.5545],\n",
            "        [-0.3736, -0.4678, -0.2156, -0.8034, -0.3715, -0.5443, -0.9146, -0.0559,\n",
            "         -0.3290, -0.2102,  0.1166, -0.1798, -0.2820, -0.3320, -0.4596, -0.1325],\n",
            "        [-0.3146,  0.0845, -0.1235, -0.7058, -0.1802,  0.5492, -0.8980, -0.4938,\n",
            "          0.6791,  0.8827,  0.4911,  0.5190,  0.9011,  0.0913, -0.1933, -0.6770],\n",
            "        [ 0.0239,  0.0998, -0.1871, -0.0860, -0.4881, -1.6765,  0.2413,  0.7361,\n",
            "          0.4608, -0.8722, -0.4259, -1.1347, -1.0571, -0.9401,  0.1343, -0.0157],\n",
            "        [-0.2362, -0.7873, -0.3802,  0.5815, -0.3722,  1.2405, -0.7004, -1.4917,\n",
            "          0.7678,  0.3584,  0.6120, -0.0794,  0.5983,  0.2635,  0.6490,  0.0709],\n",
            "        [-0.7941, -0.1660, -0.2810, -0.1021, -0.7352, -0.7518, -0.1276, -0.0051,\n",
            "          0.3325, -0.3374,  0.1678,  0.3105,  0.2258,  0.1243,  0.4617,  0.2016],\n",
            "        [ 0.1651, -0.1599, -0.5717, -0.3957,  0.3930, -0.8567,  0.3390, -0.7977,\n",
            "          0.2213, -0.5161,  0.1850, -0.2105,  0.3779,  0.0482, -0.4744, -0.0504]],\n",
            "       grad_fn=<SelectBackward0>)}\n",
            "key shape \n",
            " {torch.Size([4, 8, 16])}\n",
            "---\n",
            "wei \n",
            " {tensor([[-1.7629, -1.3011,  0.5652,  2.1616, -1.0674,  1.9632,  1.0765, -0.4530],\n",
            "        [-3.3334, -1.6556,  0.1040,  3.3782, -2.1825,  1.0415, -0.0557,  0.2927],\n",
            "        [-1.0226, -1.2606,  0.0762, -0.3813, -0.9843, -1.4303,  0.0749, -0.9547],\n",
            "        [ 0.7836, -0.8014, -0.3368, -0.8496, -0.5602, -1.1701, -1.2927, -1.0260],\n",
            "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,  0.8638,  0.3719,  0.9258],\n",
            "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,  1.4187,  1.2196],\n",
            "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,  0.8048],\n",
            "        [-1.8044, -0.4126, -0.8306,  0.5899, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
            "       grad_fn=<SelectBackward0>)}\n",
            "wei shape \n",
            " {torch.Size([4, 8, 8])}\n",
            "---\n",
            "tril \n",
            " {tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])}\n",
            "tril shape \n",
            " {torch.Size([8, 8])}\n",
            "---\n",
            "wei \n",
            " {tensor([[-1.7629,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-3.3334, -1.6556,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-1.0226, -1.2606,  0.0762,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "        [ 0.7836, -0.8014, -0.3368, -0.8496,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,    -inf,    -inf,    -inf],\n",
            "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,    -inf,    -inf],\n",
            "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,    -inf],\n",
            "        [-1.8044, -0.4126, -0.8306,  0.5899, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
            "       grad_fn=<SelectBackward0>)}\n",
            "wei shape \n",
            " {torch.Size([4, 8, 8])}\n",
            "---\n",
            "wei softmax \n",
            " {tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
            "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
            "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
            "       grad_fn=<SelectBackward0>)}\n",
            "wei shape \n",
            " {torch.Size([4, 8, 8])}\n",
            "---\n",
            "value \n",
            " {tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n",
            "         -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n",
            "        [ 0.8321, -0.8144, -0.3242,  0.5191, -0.1252, -0.4898, -0.5287, -0.0314,\n",
            "          0.1072,  0.8269,  0.8132, -0.0271,  0.4775,  0.4980, -0.1377,  1.4025],\n",
            "        [ 0.6035, -0.2500, -0.6159,  0.4068,  0.3328, -0.3910,  0.1312,  0.2172,\n",
            "         -0.1299, -0.8828,  0.1724,  0.4652, -0.4271, -0.0768, -0.2852,  1.3875],\n",
            "        [ 0.6657, -0.7096, -0.6099,  0.4348,  0.8975, -0.9298,  0.0683,  0.1863,\n",
            "          0.5400,  0.2427, -0.6923,  0.4977,  0.4850,  0.6608,  0.8767,  0.0746],\n",
            "        [ 0.1536,  1.0439,  0.8457,  0.2388,  0.3005,  1.0516,  0.7637,  0.4517,\n",
            "         -0.7426, -1.4395, -0.4941, -0.3709, -1.1819,  0.1000, -0.1806,  0.5129],\n",
            "        [-0.8920,  0.0578, -0.3350,  0.8477,  0.3876,  0.1664, -0.4587, -0.5974,\n",
            "          0.4961,  0.6548,  0.0548,  0.9468,  0.4511,  0.1200,  1.0573, -0.2257],\n",
            "        [-0.4849,  0.1655, -0.2221, -0.1345, -0.0864, -0.6628, -0.0936,  0.1050,\n",
            "         -0.2612,  0.1854,  0.3171, -0.1393,  0.5486, -0.4086, -0.3851,  0.7106],\n",
            "        [ 0.2042,  0.3772, -1.1255,  0.3995,  0.1489,  0.3590, -0.1791,  1.3732,\n",
            "          0.1588, -0.2320,  0.1651,  0.7604,  0.3521, -1.0864, -0.7939, -0.3025]],\n",
            "       grad_fn=<SelectBackward0>)}\n",
            "value shape \n",
            " {torch.Size([4, 8, 16])}\n",
            "---\n",
            "out (matmul) \n",
            " {tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n",
            "         -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n",
            "        [ 0.6764, -0.5477, -0.2478,  0.3143, -0.1280, -0.2952, -0.4296, -0.1089,\n",
            "         -0.0493,  0.7268,  0.7130, -0.1164,  0.3266,  0.3431, -0.0710,  1.2716],\n",
            "        [ 0.4823, -0.1069, -0.4055,  0.1770,  0.1581, -0.1697,  0.0162,  0.0215,\n",
            "         -0.2490, -0.3773,  0.2787,  0.1629, -0.2895, -0.0676, -0.1416,  1.2194],\n",
            "        [ 0.1971,  0.2856, -0.1303, -0.2655,  0.0668,  0.1954,  0.0281, -0.2451,\n",
            "         -0.4647,  0.0693,  0.1528, -0.2032, -0.2479, -0.1621,  0.1947,  0.7678],\n",
            "        [ 0.2510,  0.7346,  0.5939,  0.2516,  0.2606,  0.7582,  0.5595,  0.3539,\n",
            "         -0.5934, -1.0807, -0.3111, -0.2781, -0.9054,  0.1318, -0.1382,  0.6371],\n",
            "        [ 0.3428,  0.4960,  0.4725,  0.3028,  0.1844,  0.5814,  0.3824,  0.2952,\n",
            "         -0.4897, -0.7705, -0.1172, -0.2541, -0.6892,  0.1979, -0.1513,  0.7666],\n",
            "        [ 0.1866, -0.0964, -0.1430,  0.3059,  0.0834, -0.0069, -0.2047, -0.1535,\n",
            "         -0.0762,  0.3269,  0.3090,  0.0766,  0.0992,  0.1656,  0.1975,  0.7625],\n",
            "        [ 0.1301, -0.0328, -0.4965,  0.2865,  0.2704, -0.2636, -0.0738,  0.3786,\n",
            "          0.0746,  0.0338,  0.0147,  0.3194,  0.2993, -0.1653, -0.0386,  0.3375]],\n",
            "       grad_fn=<SelectBackward0>)}\n",
            "out shape \n",
            " {torch.Size([4, 8, 16])}\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "##################### Test amr #################\n",
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337) # for reproducibility\n",
        "\n",
        "B,T,C = 4,8,32 # batch (4), time (or sequence length (8)), # of channels (32) (or features) for the input tensor\n",
        "\n",
        "# tensor x initialized with random values drawn from a normal distribution with mean 0 and variance 1.\n",
        "# Tensor shape (4, 8, 32)\n",
        "x = torch.randn(B,T,C)  # Random input of shape (B, T, C)\n",
        "print(f\"input tensor x \\n\", {x})\n",
        "print(f\"input tensor x shape \\n\", {x.shape})\n",
        "print(\"---\")\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "\n",
        "# Sets the size of each attention head.\n",
        "head_size = 16\n",
        "\n",
        "# Initializes three linear transformations without bias for the key, query, and value projections\n",
        "# in the attention mechanism. Each transformation projects the input tensor\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16) # Applies the key transformation to x, resulting in k of shape (B, T, 16).\n",
        "q = query(x) # (B, T, 16) # Applies the query transformation to x, resulting in q of shape (B, T, 16).\n",
        "print(f\"query \\n\", {q[0]})\n",
        "print(f\"query shape \\n\", {q.shape})\n",
        "print(\"---\")\n",
        "print(f\"key \\n\", {k[0]})\n",
        "print(f\"key shape \\n\", {k.shape})\n",
        "print(\"---\")\n",
        "\n",
        "\n",
        "# Performs matrix multiplication between q and the transpose of k along the last two dimensions,\n",
        "# resulting in a weight tensor wei of shape (B, T, T).\n",
        "# These weights represent the raw attention scores before normalization.\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "print(f\"wei \\n\", {wei[0]})\n",
        "print(f\"wei shape \\n\", {wei.shape})\n",
        "print(\"---\")\n",
        "\n",
        "# Creates a lower-triangular matrix of ones (and zeros elsewhere) of size (T, T).\n",
        "# This will be used to mask future time steps in the attention scores,\n",
        "# enforcing causality (each position can only attend to positions before it).\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "print(f\"tril \\n\", {tril})\n",
        "print(f\"tril shape \\n\", {tril.shape})\n",
        "print(\"---\")\n",
        "\n",
        "\n",
        "# Creates a lower-triangular matrix of ones (and zeros elsewhere) of size (T, T).\n",
        "# This will be used to mask future time steps in the attention scores, enforcing causality\n",
        "# (each position can only attend to positions before it).\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))   # B,T,T\n",
        "print(f\"wei \\n\", {wei[0]})\n",
        "print(f\"wei shape \\n\", {wei.shape})\n",
        "print(\"---\")\n",
        "\n",
        "# applying softmax to the masked attention scores along the last dimension,\n",
        "# converting them into proper probability distributions.\n",
        "# The softmax function effectively nullifies the positions set to negative infinity by converting them to zero.\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "print(f\"wei softmax \\n\", {wei[0]})\n",
        "print(f\"wei shape \\n\", {wei.shape})\n",
        "print(\"---\")\n",
        "\n",
        "# Applies the value transformation to x, resulting in v of shape (B, T, 16).\n",
        "v = value(x)\n",
        "print(f\"value \\n\", {v[0]})\n",
        "print(f\"value shape \\n\", {v.shape})\n",
        "print(\"---\")\n",
        "\n",
        "# Multiplies the normalized attention weights wei with the value tensor v, resulting in the output tensor out of shape (B, T, 16).\n",
        "# This operation is the weighted sum step of the attention mechanism.\n",
        "out = wei @ v\n",
        "print(f\"out (matmul) \\n\", {out[0]})\n",
        "print(f\"out shape \\n\", {out.shape})\n",
        "print(\"---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qsyBEFfI3soa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCSXaWP5J-r5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72edce77-3226-41d5-c171-1337134bde68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei \n",
            " {tensor([[ 1.9857, -1.7370,  0.4189,  0.2985, -0.5451,  0.4942, -0.7267, -0.4810],\n",
            "        [-0.6151,  0.7704,  0.1215,  0.1193, -1.0559, -0.1234,  0.3918, -0.2687],\n",
            "        [ 0.4511,  0.6600,  0.8736, -0.5065,  0.8595,  0.2483,  0.7095, -0.3241],\n",
            "        [ 1.4350, -0.5599,  1.2163, -0.0813,  1.7313,  0.3421, -0.3146, -0.9178],\n",
            "        [-2.0204,  1.8716, -1.1214, -0.1317, -0.4320,  0.8461,  1.0991,  1.8651],\n",
            "        [ 1.0000,  0.5394,  0.9807, -0.0900,  0.7364,  1.3018,  1.4779,  1.2385],\n",
            "        [ 1.0542, -0.5249,  0.1258, -0.0781,  0.8236, -1.0546,  0.3601, -0.5679],\n",
            "        [ 0.2587,  0.1620,  0.6471,  0.2837,  1.2641,  0.3890, -0.6218, -0.4601]])}\n",
            "wei shape \n",
            " {torch.Size([4, 8, 8])}\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# scaling to stabilise the gradients\n",
        "# The scores are scaled down by dividing them by the square root of the dimension of the key vectors.\n",
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n",
        "print(f\"wei \\n\", {wei[0]})\n",
        "print(f\"wei shape \\n\", {wei.shape})\n",
        "print(\"---\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: `wei = q @ k.transpose(-2, -1) * head_size**-0.5`\n",
        "Scales the dot product by the square root of the inverse of head_size. This is done to stabilise the gradients."
      ],
      "metadata": {
        "id": "vz-Ytbl1DQ7y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3RTps1mJ-r5",
        "outputId": "a5cee62c-ebdf-4860-b73b-2cb478f24a79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# computes the variance of all elements in the tensor k\n",
        "k.var()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: This function is useful for understanding the spread or dispersion of values within the tensor, which can provide insights into the distribution and characteristics of the data."
      ],
      "metadata": {
        "id": "dMTqIhEGD7FU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPM0sJYkJ-r5",
        "outputId": "fef611de-b111-4d03-dfde-957cc8adb319"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# computes the variance of all elements in the tensor q\n",
        "q.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkJa1VaRJ-r6",
        "outputId": "77528a50-23c5-4534-e9b7-45b7dd415872"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# computes the variance of all elements in the tensor wei (the attention tensor)\n",
        "wei.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODbO4KR0J-r6",
        "outputId": "9f1d6717-684b-4f3f-c05f-43c2e7781af0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_lvQfrDJ-r6",
        "outputId": "434b0a8f-0adf-4253-b6b8-0d8a368af493"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ij5lbJ4SJ-r6",
        "outputId": "606caf77-809f-4811-a0f7-c4ffd236979b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: This code defines a class LayerNorm1d that implements Layer Normalization for a 1-dimensional input tensor.\n",
        "\n",
        "Here is an explanation of the code:\n",
        "\n",
        "- **Initialization**: The `__init__` method initializes the `LayerNorm1d` object with parameters `dim` (dimension of the input), `eps`(epsilon for numerical stability), and `momentum` (for updating running mean and variance, although not used in this implementation). It sets `gamma` to a tensor of ones and `beta` to a tensor of zeros, which are learnable parameters for scaling and shifting the normalized input.\n",
        "- **Forward Pass**: The `__call__` method computes the forward pass of the layer normalization. It calculates the mean (`xmean`) and variance (`xvar`) along the specified dimension (dimension 1 in this case, typically the feature dimension). It then normalizes the input `x` by subtracting the mean and dividing by the square root of the variance plus epsilon to ensure numerical stability. Finally, it scales (`gamma`) and shifts (`beta`) the normalized input to produce the output (`self.out`).\n",
        "- **Parameters Method**: The parameters method returns a list containing the learnable parameters of the layer normalization, which are `gamma` and `beta`. These parameters are typically updated during training through backpropagation.\n",
        "\n",
        "In summary, this code defines a simple implementation of Layer Normalization for 1-dimensional input tensors, providing a way to normalize and scale/shift features within a neural network layer."
      ],
      "metadata": {
        "id": "utmezgdVtXOR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06k0lpplJ-r7",
        "outputId": "94dbb58b-b089-41ce-9852-1a67d64db802"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_o7Tno5DJ-r7",
        "outputId": "be34a17b-37a4-4ff5-a953-d1a59e26b123"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dM17Z7hAJ-r7"
      },
      "outputs": [],
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTdl1JFmJ-r7"
      },
      "source": [
        "### Full finished code, for reference\n",
        "\n",
        "You may want to refer directly to the git repo instead though.\n",
        "\n",
        "https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvYgaVvHJ-r7",
        "outputId": "d63ad84c-cf5f-4798-ac37-67073fecf301"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.209729 M parameters\n",
            "step 0: train loss 4.4116, val loss 4.4022\n",
            "step 100: train loss 2.6568, val loss 2.6670\n",
            "step 200: train loss 2.5091, val loss 2.5060\n",
            "step 300: train loss 2.4199, val loss 2.4337\n",
            "step 400: train loss 2.3500, val loss 2.3563\n",
            "step 500: train loss 2.2961, val loss 2.3126\n",
            "step 600: train loss 2.2408, val loss 2.2501\n",
            "step 700: train loss 2.2053, val loss 2.2187\n",
            "step 800: train loss 2.1636, val loss 2.1870\n",
            "step 900: train loss 2.1226, val loss 2.1483\n",
            "step 1000: train loss 2.1017, val loss 2.1283\n",
            "step 1100: train loss 2.0683, val loss 2.1174\n",
            "step 1200: train loss 2.0376, val loss 2.0798\n",
            "step 1300: train loss 2.0256, val loss 2.0645\n",
            "step 1400: train loss 1.9919, val loss 2.0362\n",
            "step 1500: train loss 1.9696, val loss 2.0304\n",
            "step 1600: train loss 1.9625, val loss 2.0470\n",
            "step 1700: train loss 1.9402, val loss 2.0119\n",
            "step 1800: train loss 1.9085, val loss 1.9957\n",
            "step 1900: train loss 1.9080, val loss 1.9869\n",
            "step 2000: train loss 1.8834, val loss 1.9941\n",
            "step 2100: train loss 1.8727, val loss 1.9758\n",
            "step 2200: train loss 1.8585, val loss 1.9622\n",
            "step 2300: train loss 1.8537, val loss 1.9503\n",
            "step 2400: train loss 1.8419, val loss 1.9424\n",
            "step 2500: train loss 1.8153, val loss 1.9407\n",
            "step 2600: train loss 1.8267, val loss 1.9374\n",
            "step 2700: train loss 1.8126, val loss 1.9344\n",
            "step 2800: train loss 1.8054, val loss 1.9230\n",
            "step 2900: train loss 1.8045, val loss 1.9339\n",
            "step 3000: train loss 1.7963, val loss 1.9243\n",
            "step 3100: train loss 1.7691, val loss 1.9208\n",
            "step 3200: train loss 1.7506, val loss 1.9092\n",
            "step 3300: train loss 1.7548, val loss 1.9038\n",
            "step 3400: train loss 1.7582, val loss 1.8960\n",
            "step 3500: train loss 1.7376, val loss 1.8934\n",
            "step 3600: train loss 1.7232, val loss 1.8888\n",
            "step 3700: train loss 1.7280, val loss 1.8814\n",
            "step 3800: train loss 1.7221, val loss 1.8951\n",
            "step 3900: train loss 1.7228, val loss 1.8789\n",
            "step 4000: train loss 1.7168, val loss 1.8635\n",
            "step 4100: train loss 1.7168, val loss 1.8798\n",
            "step 4200: train loss 1.7088, val loss 1.8672\n",
            "step 4300: train loss 1.6995, val loss 1.8501\n",
            "step 4400: train loss 1.7096, val loss 1.8686\n",
            "step 4500: train loss 1.6907, val loss 1.8546\n",
            "step 4600: train loss 1.6868, val loss 1.8348\n",
            "step 4700: train loss 1.6786, val loss 1.8346\n",
            "step 4800: train loss 1.6659, val loss 1.8445\n",
            "step 4900: train loss 1.6711, val loss 1.8384\n",
            "step 4999: train loss 1.6630, val loss 1.8230\n",
            "\n",
            "ROMEO:\n",
            "But you far you\n",
            "my swap with thus; come hath I uD\n",
            "If sleemition of where's granded\n",
            "Of their of tout the gortune upwon alond, liege man to is Iell this surpe\n",
            "And than sleue thus mind, his by blow,\n",
            "Virdty toward butied, Ditire spresiss with thou some not.\n",
            "\n",
            "LORIO:\n",
            "I am part\n",
            "But thou sging them but\n",
            "shat secondes morry thou sovore.\n",
            "\n",
            "ISABUS:\n",
            "What art sade but hither, thange e'en,\n",
            "Protes as kingle me; an your tords whom are Ineal.\n",
            "\n",
            "MENENIUS:\n",
            "But little sweet, hom, foust cerfort;\n",
            "Winth hing diend enirs' tompy beds sick ways!\n",
            "What curforself this grace. Won, passes us.\n",
            "\n",
            "BUCKINGHABY MARD:\n",
            "Mether star: keep it any head which\n",
            "He tall devioly that, out that confer old.\n",
            "Our thy dears time.\n",
            "Nay, the fragoly, pair, of new\n",
            "my father, my lip Backnoward:\n",
            "God therring for respide\n",
            "What colvery, teminelyord, I mast,\n",
            "While us that such differs I'll that confect I come,\n",
            "But; man.\n",
            "\n",
            "VOLUMNIO:\n",
            "Ontread confail with me. Humser dipporbried answeraw is codal one,\n",
            "Onjestion, not or cheavess ensty with.\n",
            "\n",
            "GLOUCESTER:\n",
            "\n",
            "HENRY Mess to Lies?\n",
            "Stand and these beguare youf stile that than war\n",
            "offity are, I usquesch\n",
            "Frown movhapty not duke with you addom\n",
            "grack prowd--lost\n",
            "But but they worse is senst my crunne undolier. But, beauts pruntaly; I stoll'ct my nor Murder, I sot, though who speak\n",
            "Your bout told-man rathing if anyshal\n",
            "epitence, tirre no the said he's,\n",
            "Andis frultifs. what his lide? That mirdy this dudgetions?\n",
            "\n",
            "KING ARINIA:\n",
            "I let holt not sucKether,\n",
            "Whither, efore But lord: I, beget because at that his say\n",
            "as to brought grave a donesmer all nobe.\n",
            "\n",
            "BUCKINGHUMBY:\n",
            "Which forgeled! Came; nor thereforn's fiends strefet.\n",
            "\n",
            "PLORIA:\n",
            "Yet to Capprohning, that brird\n",
            "of say mover a desrick.\n",
            "\n",
            "MO\n",
            "stompars, God the\n",
            "citchard is high.\n",
            "\n",
            "Seth Second Methere:\n",
            "Marrmat I unmale the bretcius unfoect that I would back where own thy lurges\n",
            "And, iffillimorture:\n",
            "As thou twand, York these that high praut.\n",
            "Plafe merprates sure dread with her,\n",
            "At not your must I suchon? too prant!\n",
            "O 'hiles clight the bleave is graved before\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_Nvn2pJJSSx"
      },
      "source": [
        "# FULL CODE EXPLANATION\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoQduAIPRHVw"
      },
      "source": [
        "## Hyperparameters:\n",
        "\n",
        "```\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "```\n",
        "\n",
        "\n",
        "- `batch_size = 16`: The number of sequences processed in parallel during training. Batch size affects both learning dynamics and computational efficiency.\n",
        "- `block_size = 32`: The maximum sequence length the model will consider for predictions. In the context of Transformers, this is often referred to as the model's \"context size\" or \"sequence length,\" limiting how much prior context the model can use.\n",
        "- `max_iters = 5000`: The maximum number of iterations (or updates) to perform during training. This caps the training process and is crucial for preventing overfitting and unnecessary computation.\n",
        "- `eval_interval = 100`: The interval (in iterations) at which the model's performance is evaluated on a validation set or some evaluation metric is calculated. This helps in monitoring the training progress.\n",
        "- `learning_rate = 1e-3`: The step size used for updating the model's weights during optimization. The learning rate is a critical hyperparameter that affects training stability and convergence.\n",
        "- `device = 'cuda' if torch.cuda.is_available() else 'cpu'`: Determines the computing device for training. If CUDA (NVIDIA's GPU computing platform) is available, training will be accelerated using the GPU; otherwise, it falls back to the CPU.\n",
        "- `eval_iters = 200`: This parameter might indicate the number of iterations to run during each evaluation phase, but its specific role depends on the context in which it's used in the training or evaluation loop.\n",
        "- `n_embd = 64`: The size of the embeddings used for representing tokens. This could also imply the dimensionality of the model's hidden layers.\n",
        "- `n_head = 4`: The number of attention heads in each Transformer block. Multi-head attention allows the model to focus on different parts of the input sequence simultaneously.\n",
        "- `n_layer = 4`: The number of layers (or depth) of the Transformer model. More layers can increase the model's capacity but also its computational cost and the risk of overfitting.\n",
        "- `dropout = 0.0`: The dropout rate used for regularization during training. Dropout randomly zeroes some of the elements of the input tensor with the given probability, helping prevent overfitting. A value of 0.0 indicates no dropout is applied.\n",
        "\n",
        "Setting a Manual Seed:\n",
        "\n",
        "`torch.manual_seed(1337)`: Sets the seed for generating random numbers in PyTorch. This ensures that the model's initialization and any other random operations are reproducible across runs for debugging and comparison purposes.\n",
        "\n",
        "These parameters together define the structure and training behavior of a Transformer-based model. Adjusting these hyperparameters can significantly affect model performance, training speed, and resource requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnc7b2X7S2tX"
      },
      "source": [
        "## Function estimate loss\n",
        "The code defines a function `estimate_loss` that estimates the **average loss of a model on the training and validation datasets without updating the model's weights**. It's designed to be run during or after training to monitor the model's performance.\n",
        "\n",
        "\n",
        "```\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Let's break down what it does:\n",
        "\n",
        "`@torch.no_grad()` Decorator:\n",
        "\n",
        "- This decorator is applied to the estimate_loss function to disable gradient computation within the function. Disabling gradients saves memory and computations, making evaluation faster since backpropagation (needed for training) is not performed.\n",
        "\n",
        "Function Definition `def estimate_loss()`:\n",
        "\n",
        "- Defines the function `estimate_loss` that, when called, estimates and returns the average loss for both the training and validation splits.\n",
        "\n",
        "- Initialization of the Output Dictionary:\n",
        "\n",
        "- `out = {}` initializes an empty dictionary to store the average loss for each data split ('train' and 'val').\n",
        "\n",
        "Model Evaluation Mode:\n",
        "\n",
        "- `model.eval()` sets the model to evaluation mode, affecting layers like dropout and batch normalization, which behave differently during training vs. evaluation.\n",
        "\n",
        "Iterating Over Data Splits:\n",
        "\n",
        "- The loop for split in `['train', 'val']`: iterates over two splits: training and validation.\n",
        "\n",
        "Loss Calculation for Each Split:\n",
        "\n",
        "- Initializes a tensor losses = `torch.zeros(eval_iters)` to store the loss values for each iteration in evaluating the specified split.\n",
        "\n",
        "- Iterates `eval_iters` times, each time retrieving a batch of data `X`, `Y` through `get_batch(split)`, which is assumed to be a function that provides batches of input data `X` and target labels `Y` for the specified split.\n",
        "\n",
        "- `logits, loss = model(X, Y)` calculates the model's output logits and the corresponding loss `loss` for the given batch. The model is expected to return both the raw predictions (`logits`) and the calculated loss value when provided with inputs and targets.\n",
        "\n",
        "- `losses[k] = loss.item()` stores the loss value for the current iteration by converting the loss tensor to a Python scalar using `.item()`.\n",
        "\n",
        "Storing the Mean Loss:\n",
        "\n",
        "- After iterating through `eval_iters` batches, the mean loss for the split is calculated as `losses.mean()` and stored in the `out` dictionary with the split name as the key.\n",
        "\n",
        "Switching Back to Training Mode:\n",
        "\n",
        "- `model.train()` sets the model back to training mode, re-enabling the training-specific behaviors like dropout and batch normalization updates, which were disabled during evaluation.\n",
        "\n",
        "Return Statement:\n",
        "\n",
        "- Finally, the function returns the `out` dictionary containing the average loss for both the training and validation splits.\n",
        "\n",
        "This function is useful for monitoring model performance without affecting its training state, providing insights into how well the model is learning and generalizing to unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCL4MlgiUjGi"
      },
      "source": [
        "## Class Head\n",
        "\n",
        "This code defines a class `Head`, which represents a **single head of self-attention** within a Transformer architecture. The self-attention mechanism allows the model to weigh the importance of different parts of the input sequence differently for each element in the sequence. This class is implemented as a subclass of nn.Module, which is the base class for all neural network modules in PyTorch.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "```\n",
        "Let's break down its components:\n",
        "\n",
        "`Initialization Method __init__(self, head_size)`\n",
        "\n",
        "Parameters:\n",
        "- `head_size`: The size of the head, which determines the dimensionality of the key, query, and value vectors.\n",
        "\n",
        "Attributes:\n",
        "- `self.key`, `self.query`, `self.value`: These are linear layers (without bias) that transform the input tensor `x` into key, query, and value representations, respectively.\n",
        "- Each transformation projects the input embeddings (`n_embd`) into a lower-dimensional space (`head_size`).\n",
        "- `self.tril`: A lower triangular matrix registered as a buffer. This matrix is used for masking to ensure that each position in the sequence can only attend to preceding positions, enforcing causality in the attention mechanism. The use of `torch.tril(torch.ones(block_size, block_size))` creates a square matrix where each element below the main diagonal is 1 (inclusive), and all others are 0.\n",
        "- `self.dropout`: A dropout layer applied to the attention weights to prevent overfitting by randomly setting elements of the attention matrix to zero during training.\n",
        "\n",
        "Forward Method `forward(self, x)`:\n",
        "\n",
        "Input:\n",
        "- `x`: The input tensor with shape (`B, T, C`), where `B` is the batch size, `T` is the sequence length, and `C` is the number of features (`embedding dimension`).\n",
        "\n",
        "Process:\n",
        "- The input `x` is first transformed into key (`k`), query (`q`), and value (`v`) representations using the linear transformations defined in `__init__`.\n",
        "\n",
        "- Attention scores are computed by taking the dot product of queries and keys `(q @ k.transpose(-2,-1))` and then scaling by the inverse square root of the dimensionality `(C**-0.5)`. This scaling factor is used to stabilize gradients.\n",
        "\n",
        "- The resulting attention scores are masked with `-inf` where the lower triangular matrix `(self.tril)` is zero, ensuring causality. This masking before the softmax operation effectively removes these positions from consideration by making their weights zero after softmax is applied.\n",
        "\n",
        "- A softmax function is applied to the masked attention scores along the last dimension to obtain the final attention weights, which are then passed through a dropout layer.\n",
        "\n",
        "- The attention weights are used to perform a weighted aggregation of the value vectors, producing the output of the self-attention head.\n",
        "\n",
        "Output:\n",
        "\n",
        "- The method returns the output tensor `out`, which is the result of applying self-attention to the input. This tensor has the same shape as the input (`B, T, C`) and contains the aggregated information based on the computed attention weights.\n",
        "\n",
        "This class encapsulates the functionality of a single attention head, focusing on computing weighted sums of value vectors based on the similarity between queries and keys, while respecting the sequential nature of the input through masking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtP9rscCWuUq"
      },
      "source": [
        "## MultiHead Attention\n",
        "\n",
        "This code defines a class MultiHeadAttention, which represents a multi-head self-attention mechanism within a Transformer model. **Multi-head attention allows the model to simultaneously attend to information from different representation subspaces at different positions**. Here's a detailed breakdown:\n",
        "\n",
        "\n",
        "```\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_heads * head_size, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "```\n",
        "Here's a detailed breakdown:\n",
        "\n",
        "\n",
        "Class Definition and Initialization Method `__init__(self, num_heads, head_size)`:\n",
        "\n",
        "Parameters:\n",
        "\n",
        "- `num_heads`: The number of parallel attention heads.\n",
        "- `head_size`: The size of each attention head.\n",
        "\n",
        "Attributes:\n",
        "- `self.heads`: A nn.ModuleList containing instances of the Head class (defined previously). Each Head instance represents a single self-attention head. The list size is determined by `num_heads`, allowing for parallel computation of different attention \"views.\"\n",
        "- `self.proj`: A linear layer that projects the concatenated output of all attention heads back to the original embedding dimension (n_embd). This projection is necessary because the concatenated outputs from all heads increase the dimensionality, and we often want the output of the multi-head attention to have the same dimensionality as the input for residual connections and further processing.\n",
        "- `self.dropout`: A dropout layer applied after the projection to prevent overfitting by randomly zeroing out elements of the output tensor during training.\n",
        "\n",
        "Forward Method `forward(self, x)`:\n",
        "\n",
        "Input:\n",
        "\n",
        "- `x`: The input tensor with shape (`B, T, C`), where `B` is the batch size, `T` is the sequence length, and `C` is the number of features (embedding dimension).\n",
        "\n",
        "Process:\n",
        "\n",
        "- The input `x` is processed by each attention head in `self.heads`, resulting in `num_heads` output tensors.\n",
        "- These outputs are concatenated along the last dimension `(dim=-1)`. Since each head potentially transforms the input into a different subspace (`head_size`), concatenating these allows the model to combine diverse information from different subspaces.\n",
        "\n",
        "- The concatenated output is then projected back to the original embedding dimension (`n_embd`) using `self.proj`.\n",
        "Dropout is applied to the projected output as a regularization measure.\n",
        "\n",
        "Output:\n",
        "\n",
        "- The method returns the final output tensor, which has undergone multi-head self-attention and been projected back to the original embedding dimensionality. This output can be used for further processing or as part of a larger model, like a Transformer block.\n",
        "\n",
        "This class effectively combines information from multiple perspectives (`heads`) on the input sequence, enhancing the model's ability to capture various dependencies and features within the data. Multi-head attention is a key component of Transformer architectures, contributing to their effectiveness in handling complex sequence-based tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzffiDgiY8S_"
      },
      "source": [
        "## Block(nn.module)\n",
        "This code defines a class called Block, which represents a single block within a transformer.\n",
        "The number of blocks can be modified to build a network\n",
        "\n",
        "Each block contains:\n",
        "- Layer Norm\n",
        "- Multi-headed attention\n",
        "- A skip connection\n",
        "- Second layer norm\n",
        "- Feed Forward network\n",
        "- Another skip connection\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "```\n",
        "Initialization (__init__ method):\n",
        "\n",
        "- The `__init__ method` initializes the Block class with two parameters: n_embd (embedding dimension) and n_head (the number of attention heads).\n",
        "Inside the __init__ method:\n",
        "  - The head_size is computed as the embedding dimension divided by the number of attention heads.\n",
        "  - self.sa (self attentions) is initialized as an instance of MultiHeadAttention, which is a multi-head attention mechanism with n_head heads and head_size as the size of each head.\n",
        "  - self.ffwd is initialized as an instance of FeedForward, which likely represents a feedforward neural network component within the block.\n",
        "self.ln1 and self.ln2 are initialized as instances of\n",
        "- LayerNorm, which are layer normalization layers applied before and after the attention and feedforward layers, respectively.\n",
        "- Forward pass (forward method):\n",
        "\n",
        "- The `forward` method defines the forward pass computation for the Block.\n",
        "Inside the forward method:\n",
        "  - The input tensor x is first normalized using `self.ln1`.\n",
        "  - The normalized tensor is passed through the multi-head attention mechanism (`self.sa`), and the result is added to the original input tensor `x`.\n",
        "  - The sum is then normalized again using `self.ln2`.\n",
        "  - The normalized tensor is passed through the feedforward neural network (`self.ffwd`), and the result is added to the tensor obtained from the previous step.\n",
        "- The final result is returned as the output of the block.\n",
        "\n",
        "In summary, the `Block` class represents a single transformer block, which consists of a multi-head attention mechanism followed by a feedforward neural network, with layer normalization applied before and after each sub-module. This structure is a key component of transformer architectures used in natural language processing and other sequence modeling tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxSntzrnFget"
      },
      "source": [
        "## BigramLanguageModel\n",
        "This code defines a `BigramLanguageModel` class, which is a simplified transformer-based model intended for language modeling tasks. The model aims to predict the next token in a sequence based on the previous tokens, implementing a structure reminiscent of Transformer models but tailored for a specific context of generating or evaluating sequences token-by-token.\n",
        "\n",
        "```\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "```\n",
        "Let's dissect its components and functionalities:\n",
        "\n",
        "- Initialization Method `__init__(self)`:\n",
        "\n",
        "Attributes:\n",
        "\n",
        "- `self.token_embedding_table`: An embedding layer for tokens. It maps tokens from a vocabulary of size vocab_size to embeddings of size n_embd.\n",
        "- `self.position_embedding_table`: An embedding layer for positions within a sequence. It maps position indices (up to block_size) to embeddings of the same size n_embd, enabling the model to understand the order of tokens.\n",
        "- `self.blocks`: A sequential container of Block instances. Each Block is presumably a Transformer block (not defined in this snippet), which includes self-attention and feedforward layers, applied successively to the input embeddings. The number of blocks is n_layer, allowing for multiple layers of processing.\n",
        "- `self.ln_f`: A layer normalization applied to the final output of the Transformer blocks. It helps stabilize the hidden state distributions before the final prediction.\n",
        "- `self.lm_head`: A linear projection layer that maps the output of the Transformer blocks back to the vocabulary space. This is used to produce logits for each token in the vocabulary.\n",
        "\n",
        "Forward Method `forward(self, idx, targets=None)`:\n",
        "\n",
        "Inputs:\n",
        "\n",
        "-`idx`: A tensor of shape (B, T) containing indices of input tokens.\n",
        "- `targets`: An optional tensor of shape (B, T) containing indices of target tokens for training. If `targets` is not provided, the method assumes inference mode and does not compute loss.\n",
        "\n",
        "Process:\n",
        "\n",
        "- Embeds both tokens and their positions, then sums these embeddings to produce a representation that contains information about both the identity and the order of tokens.\n",
        "- Processes the embedded input through the Transformer blocks `(self.blocks)`.\n",
        "- Applies layer normalization `(self.ln_f)`.\n",
        "- Projects the normalized output to the vocabulary space `(self.lm_head)`, producing logits for each token position.\n",
        "- If `targets` are provided, computes the cross-entropy loss between the predicted logits and the true targets. This is done after reshaping logits and targets to ensure compatibility with the loss function.\n",
        "\n",
        "Outputs:\n",
        "\n",
        "- `logits`: The logits corresponding to the probability distribution over the vocabulary for each token position.\n",
        "- `loss`: The computed cross-entropy loss if targets are provided, otherwise None.\n",
        "\n",
        "Generate Method `generate(self, idx, max_new_tokens)`:\n",
        "\n",
        "Inputs:\n",
        "\n",
        "`idx`: A tensor of shape (B, T) containing the starting sequence of token indices.\n",
        "- `max_new_tokens`: The maximum number of new tokens to generate.\n",
        "\n",
        "Process:\n",
        "\n",
        "- Iteratively generates `max_new_tokens` by predicting one token at a time and appending it to the sequence.\n",
        "- At each step, limits the context to the last `block_size` tokens to manage computation and memory efficiency, and to adhere to the model's design constraints.\n",
        "- Uses the model's forward pass to get logits for the next token, applies `softmax` to convert logits into probabilities, and then samples a new token index from this probability distribution.\n",
        "- Concatenates the newly sampled token to the existing sequence and repeats until max_new_tokens are generated.\n",
        "\n",
        "Output:\n",
        "\n",
        "- Returns the extended sequence with the newly generated tokens appended.\n",
        "\n",
        "This class encapsulates the functionalities required for both training a language model (via the forward method) and generating text (via the generate method), showcasing a basic yet powerful application of Transformer architecture principles in NLP tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mGWQFnRiWIwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iebQCgmqODlH"
      },
      "source": [
        "\n",
        "## Training Loop\n",
        "\n",
        "It code below outlines the procedure for training the model on a dataset, evaluating its performance periodically, and finally generating text based on a given context.\n",
        "\n",
        "```\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n",
        "```\n",
        "\n",
        "Let's break down the code into its key components:\n",
        "\n",
        "**Training Loop**\n",
        "\n",
        "Iteration Loop:\n",
        "\n",
        "- `for iter in range(max_iters)`: iterates through a specified number of training iterations or epochs (`max_iters`).\n",
        "\n",
        "Conditional Evaluation:\n",
        "\n",
        "- `if iter % eval_interval == 0 or iter == max_iters - 1`: checks if the current iteration is a multiple of the evaluation interval (eval_interval) or the last iteration. If so, the model's performance is evaluated.\n",
        "\n",
        "- `losses = estimate_loss()` calls a function to estimate the model's loss on both the training and validation datasets. This function is likely similar to the estimate_loss function explained earlier.\n",
        "The training and validation losses are printed to monitor the model's performance over time.\n",
        "Batch Processing:\n",
        "\n",
        "`xb, yb = get_batch('train')` fetches a batch of input data (`xb`) and corresponding targets (`yb`) for training.\n",
        "\n",
        "Loss Calculation and Optimization:\n",
        "\n",
        "- `logits, loss = model(xb, yb)` computes the model's predictions (`logits`) and the loss (`loss`) for the current batch.\n",
        "\n",
        "- `optimizer.zero_grad(set_to_none=True)` clears any old gradients from the previous step to prevent accumulation.\n",
        "\n",
        "- `loss.backward()` computes the gradient of the loss with respect to the model parameters.\n",
        "\n",
        "- `optimizer.step()` updates the model parameters based on the gradients.\n",
        "\n",
        "**Text Generation**\n",
        "\n",
        "Initialization:\n",
        "\n",
        "`context = torch.zeros((1, 1), dtype=torch.long, device=device)` initializes a tensor to serve as the starting context for text generation. It's set to a single token of zeros, indicating an empty context or start token.\n",
        "\n",
        "Generation Loop:\n",
        "\n",
        "- `m.generate(context, max_new_tokens=2000)` generates new tokens starting from the provided context for a maximum of 2000 new tokens. The generate method likely uses the model to predict the next token based on the current sequence, samples a token from the predicted probabilities, and appends it to the sequence. This process is repeated until the maximum number of new tokens is reached.\n",
        "\n",
        "Decoding and Printing:\n",
        "\n",
        "- `decode(m.generate(context, max_new_tokens=2000)[0].tolist())` decodes the generated indices back into readable text. The decode function is assumed to map numerical token IDs back to their corresponding string representations. The generated text is then printed.\n",
        "\n",
        "This training and generation loop is typical for neural network-based language models, combining periods of training with evaluation to monitor progress and adjusting model parameters to minimize loss. The final generation step showcases the model's ability to produce coherent and contextually relevant text sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IofAbhykZJAZ"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "With the above, we implemented a **decoder-only transformer model**. It is a decoder, because we apply triangular masking to the attention matrix `wei`. The encoder part is missing.<br>\n",
        "Our model only consists of the Self-Attention block and the Feed-Forward block. The 'encoder-fusing' cross-attention part is missing.<br>\n",
        "\n",
        "We are just generating text based on an initial prompt, thus basically 'blabbering on' without further reference to anything.<br>\n",
        "\n",
        "The original paper proposes an encoder-decoder architecture for machine translation:\n",
        "- The encoder part is a transformer model, too. It is trained to encode the source language into a fixed-size vector\n",
        "- The decoder part is trained to decode the fixed-size vector into the target language\n",
        "- Both encoder and decoder are trained to work together\n",
        "\n",
        "| Encode | Decode |\n",
        "| --- | --- |\n",
        "| les chats sont mignons | <START>the cats are cute<END> |\n",
        "\n",
        "Unlike what we did, the generation will be influenced by additional information, meaning the source language input.<br>\n",
        "Within the encoder, **no** triangular masking is applied. The encoder is allowed to look at all the input tokens, time-wise.<br>\n",
        "Feeding that into the decoder, the decoder is conditioned not only on the past of the current decoding, but also on the fully encoded input.<br>\n",
        "\n",
        "\n",
        "### ChatGPT?!\n",
        "\n",
        "How does ChatGPT relate to what we did here?<br>\n",
        "To achieve a system like ChatGPT, two steps have to be taken: Pre-Training a Decoder-Only Transformer to 'babbel text' and then fine-tuning it on prompt-response pairs.<br>\n",
        "\n",
        "We have around $300,000$ tokens in our dataset. OpenAI has around $300,000,000,000$ tokens in their training dataset. And that is still small by today's (trillions) standards.<br>\n",
        "\n",
        "**TL;DR:** OpenAI takes the transformer and blows it up to the max training- and finetuning-wise.<br>\n",
        "The challenge OpenAI faces is thus not only one of building the AI, but maintaining an infrastructure that can handle the training and fine-tuning of such a model.\n",
        "\n",
        "To now align the decoder-only 'babbeling' transformer with a task like chat generation, we need to add the encoder part and the cross-attention part.<br>\n",
        "OpenAI does this like so:\n",
        "\n",
        "**Step 1:** Collect demonstration data and train a supervised policy\n",
        "- Collect (a way smaller set of) training data for code generation (e.g. a question and the answer set on programming tasks)\n",
        "- Fine-tune the model to focus on achieving a response that is as close as possible to the answer set. This aligns the model with the task of chat generation.\n",
        "\n",
        "**Step 2:** Collect comparison data and train a reward model\n",
        "- For one input, several iterations of outputs are collected and ranked by humans\n",
        "- This information is used to train a reward model that can rank the outputs of the model by itself on a large scale\n",
        "\n",
        "**Step 3:** Optimize a policy against the reward model using the [PPO](https://openai.com/blog/openai-baselines-ppo/) reinforcement learning algorithm\n",
        "- Again, we start with a prompt from the training data\n",
        "- Out trained (supervised policy) model takes the prompt and generates a response\n",
        "    - [PPO](https://openai.com/blog/openai-baselines-ppo/) is applied so that the outputs that are ranked higher by the reward model are more likely to be generated\n",
        "\n",
        "These steps take the model from being a 'blabby' decoder-only transformer to being a chat generator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBlQ59oEQJ0L"
      },
      "source": [
        "# Resources\n",
        "\n",
        "- [Attention is All You Need paper](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "- [OpenAI GPT-3 paper](https://arxiv.org/abs/2005.14165 )\n",
        "\n",
        "- [OpenAI ChatGPT blog post](https://openai.com/blog/chatgpt/)\n",
        "\n",
        "- [The illustrated transfomer](https://jalammar.github.io/illustrated-transformer/)\n",
        "\n",
        "- [Matrix Multiplication](http://matrixmultiplication.xyz/)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rk7MKpHoOnjs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}