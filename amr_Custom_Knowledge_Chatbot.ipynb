{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/royam0820/ChatGPT-Google/blob/main/amr_Custom_Knowledge_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6337c0b",
      "metadata": {
        "id": "a6337c0b"
      },
      "source": [
        "# Custom Knowledge Chatbot w/ LlamaIndex\n",
        "By Liam Ottley - YouTube: https://www.youtube.com/@LiamOttley\n",
        "\n",
        "https://www.youtube.com/watch?v=sUSw9MaPm2M\n",
        "\n",
        "Github: https://github.com/wombyz/custom-knowledge-chatbot/tree/main/custom-knowledge-chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8911e71",
      "metadata": {
        "id": "a8911e71"
      },
      "source": [
        "Examples:\n",
        "- https://gita.kishans.in/\n",
        "- https://www.chatpdf.com/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml6MMJFSOZOW",
        "outputId": "e5430563-8c25-4b9d-ae8c-b81b9151493a"
      },
      "id": "ml6MMJFSOZOW",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/wombyz/custom-knowledge-chatbot.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSiDXMALP7A5",
        "outputId": "2c1d7254-e2b3-4918-b7cf-d9a2bf0d9ead"
      },
      "id": "NSiDXMALP7A5",
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'custom-knowledge-chatbot'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 21 (delta 5), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (21/21), 18.11 KiB | 1.65 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c425f155",
      "metadata": {
        "id": "c425f155"
      },
      "outputs": [],
      "source": [
        "!pip install llama_index\n",
        "!pip install langchain\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **LlamaIndex** (GPT Index) is a project that provides a central interface to connect your LLM's with external data. https://gpt-index.readthedocs.io/en/latest/index.html\n",
        "- LangChain is a framework built around Large Language Models (LLMs) that can be used for various natural language processing tasks. At its core, LangChain allows developers to \"chain\" together different components to create more advanced use cases around LLMs. LangChain is a framework built around Large Language Models (LLMs) that can be used for various natural language processing tasks. At its core, LangChain allows developers to \"chain\" together different components to create more advanced use cases around LLMs  https://www.pinecone.io/learn/langchain-intro/"
      ],
      "metadata": {
        "id": "hThsMxHyvERp"
      },
      "id": "hThsMxHyvERp"
    },
    {
      "cell_type": "markdown",
      "id": "1041eae8",
      "metadata": {
        "id": "1041eae8"
      },
      "source": [
        "# Basic LlamaIndex Usage Pattern"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ec6395b7",
      "metadata": {
        "id": "ec6395b7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://platform.openai.com/account/api-keys"
      ],
      "metadata": {
        "id": "8DpxI9zTwJFu"
      },
      "id": "8DpxI9zTwJFu"
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "5cf41880",
      "metadata": {
        "id": "5cf41880"
      },
      "outputs": [],
      "source": [
        "# Load you data into 'Documents' a custom type by LlamaIndex\n",
        "\n",
        "from llama_index import SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader('/content/drive/MyDrive/chatbot_knowledge/data').load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "98df5bf6",
      "metadata": {
        "id": "98df5bf6"
      },
      "outputs": [],
      "source": [
        "# Create an index of your documents\n",
        "\n",
        "from llama_index import GPTSimpleVectorIndex\n",
        "\n",
        "index = GPTSimpleVectorIndex.from_documents(documents)\n",
        "\n",
        "#amr\n",
        "# save the index to a file\n",
        "index.save_to_disk('/content/drive/MyDrive/chatbot_knowledge/data/index_simple_vector.json')\n",
        "\n",
        "# load the index from the saved file\n",
        "index = GPTSimpleVectorIndex.load_from_disk('/content/drive/MyDrive/chatbot_knowledge/data/index_simple_vector.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the provided web search results, GPTSimpleVectorIndex is an indexing system used to store and retrieve embeddings generated by Large Language Models (LLMs) like GPT. The embeddings are created during the index construction phase and stored in a vectorized index. This index allows for efficient querying and finding relevant parts of data based on the similarity of the query and the data [1][2]. The GPTSimpleVectorIndex is one of several indexing systems available in the GPT Index project, including GPTChromaIndex and GPTTreeIndex"
      ],
      "metadata": {
        "id": "vqg4zk-L0KJ5"
      },
      "id": "vqg4zk-L0KJ5"
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "18731b6b",
      "metadata": {
        "id": "18731b6b",
        "outputId": "8697c813-c95b-4342-9aca-f41c2272455e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>\nFacebook's LLaMa appears to be a promising development in the field of AI. It is designed to help researchers advance their work in this subfield of AI by providing access to a state-of-the-art large language model. It is also designed to be versatile and can be applied to many different use cases, which is beneficial for researchers who don't have access to large amounts of infrastructure. Additionally, Facebook is taking responsible AI practices into consideration by providing a model card and releasing the model under a noncommercial license focused on research use cases.</b>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Query your index!\n",
        "\n",
        "response = index.query(\"What do you think of Facebook's LLaMa?\")\n",
        "#print(response)\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57864389",
      "metadata": {
        "id": "57864389"
      },
      "source": [
        "# Customize your LLM for different output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI"
      ],
      "metadata": {
        "id": "duE1nkub4TDb"
      },
      "id": "duE1nkub4TDb",
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boJkm_Nt1h_Z",
        "outputId": "4a6431d9-b21d-4362-a98d-bc3bcbc45961"
      },
      "id": "boJkm_Nt1h_Z",
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: openai\n",
            "Version: 0.27.2\n",
            "Summary: Python client library for the OpenAI API\n",
            "Home-page: https://github.com/openai/openai-python\n",
            "Author: OpenAI\n",
            "Author-email: support@openai.com\n",
            "License: \n",
            "Location: /usr/local/lib/python3.9/dist-packages\n",
            "Requires: aiohttp, requests, tqdm\n",
            "Required-by: llama-index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "c726dc51",
      "metadata": {
        "id": "c726dc51"
      },
      "outputs": [],
      "source": [
        "# Setup your LLM\n",
        "\n",
        "from llama_index import LLMPredictor, GPTSimpleVectorIndex, PromptHelper\n",
        "\n",
        "\n",
        "# define LLM\n",
        "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.1, model_name=\"text-davinci-002\"))\n",
        "\n",
        "\n",
        "# define prompt helper\n",
        "# set maximum input size\n",
        "max_input_size = 4096\n",
        "# set number of output tokens\n",
        "num_output = 256\n",
        "# set maximum chunk overlap\n",
        "max_chunk_overlap = 20\n",
        "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
        "\n",
        "# custom_LLM_index = GPTSimpleVectorIndex(\n",
        "#     documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper\n",
        "# )\n",
        "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load index\n",
        "custom_LLM_index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)"
      ],
      "metadata": {
        "id": "zOzsmtJZB0gR"
      },
      "id": "zOzsmtJZB0gR",
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "id": "f359b0c4",
      "metadata": {
        "id": "f359b0c4",
        "outputId": "189693f0-2dee-400a-c7c6-7c85bfff5d8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>\nI think it's a great idea!</b>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Query your index!\n",
        "\n",
        "response = custom_LLM_index.query(\"What do you think of Facebook's LLaMa?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))\n",
        "#print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "458f4e5c",
      "metadata": {
        "id": "458f4e5c"
      },
      "source": [
        "# Wikipedia Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "4db9772b",
      "metadata": {
        "id": "4db9772b"
      },
      "outputs": [],
      "source": [
        "from llama_index import download_loader\n",
        "\n",
        "WikipediaReader = download_loader(\"WikipediaReader\")\n",
        "\n",
        "loader = WikipediaReader()\n",
        "wikidocs = loader.load_data(pages=['Cyclone Freddy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Cyclone Freddy](https://en.wikipedia.org/wiki/Cyclone_Freddy)"
      ],
      "metadata": {
        "id": "41WcT3tpIPNA"
      },
      "id": "41WcT3tpIPNA"
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "941c9bf2",
      "metadata": {
        "id": "941c9bf2"
      },
      "outputs": [],
      "source": [
        "wiki_index = GPTSimpleVectorIndex.from_documents(wikidocs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "e4a4dd03",
      "metadata": {
        "id": "e4a4dd03",
        "outputId": "15dbe5f9-14da-4e5a-d3d9-6b7d267c8d52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>\n\nCyclone Freddy is a very intense tropical cyclone that traversed the southern Indian Ocean for more than five weeks in February and March 2023. It is both the longest-lasting and highest-ACE-producing tropical cyclone ever recorded worldwide. Freddy first developed as a disturbance embedded within the monsoon trough on 4 February 2023 and quickly intensified to a Category 4 severe tropical cyclone. At its peak strength, the Joint Typhoon Warning Center (JTWC) estimated 1-minute sustained winds of 270 km/h (165 mph), equivalent to Category 5 strength on the Saffir–Simpson scale. Freddy made its first landfall near Mananjary, Madagascar, and its second landfall just south of Vilankulos, Mozambique. The storm caused catastrophic flash floods in Malawi, killing at least 894 people. Strong winds and waves were observed along the northern coast of Mauritius. Winds in Port Louis reached 104 km/h (65 mph) while a peak gust of 154 km/h (96 mph) was observed on Signal Mountain. Flooding and gale-force winds also affected the country. Around 4:00 a.m. local time on 20 February, contact was lost with the Taiwanese-flagged fishing traw</b>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = wiki_index.query(\"What is cyclone freddy?\")\n",
        "#print(response)\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "outputId": "1fb26446-42a1-4b43-c57c-ddc98ecdba98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "NbK26dBZb9pz"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>\nCyclone Freddy affected Madagascar, Mozambique, Malawi, Zimbabwe, South Africa, and Eswatini, resulting in millions of people being at risk of an increase in cholera cases due to contaminated water and food, and food prices jumping by over 300%. Tanzania, Zambia, and Malawi's defense force provided helicopters to bring aid to cut off areas, and the IFRC donated 6 million swiss francs ($6,505,680) to help bring aid to 100,00 people across 5 districts. The Everlasting Life Missionary Church also donated assorted clothes and fed children at camps.</b>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = wiki_index.query(\"What country were affected by cyclone freddy?\")\n",
        "#print(response)\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "id": "NbK26dBZb9pz"
    },
    {
      "cell_type": "markdown",
      "id": "ebea2acc",
      "metadata": {
        "id": "ebea2acc"
      },
      "source": [
        "# Customer Support Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "id": "19f396a9",
      "metadata": {
        "id": "19f396a9"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader('/content/drive/MyDrive/chatbot_knowledge/asos').load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "id": "cb30944e",
      "metadata": {
        "id": "cb30944e"
      },
      "outputs": [],
      "source": [
        "index = GPTSimpleVectorIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "id": "946c44ef",
      "metadata": {
        "id": "946c44ef",
        "outputId": "dde47481-e50d-4f66-c48e-18528cb1b6db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>\nIn the United Arab Emirates, you have the option of signing up for ASOS Premier, which gives you free Standard and Express delivery all year round when you spend over 150 AED. It costs 200 AED and is valid on the order you purchase it on.</b>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = index.query(\"What premier service options do I have in the UAE?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))\n",
        "#print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e77e646",
      "metadata": {
        "id": "5e77e646"
      },
      "source": [
        "# YouTube Video Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "89160742",
      "metadata": {
        "id": "89160742"
      },
      "outputs": [],
      "source": [
        "YoutubeTranscriptReader = download_loader(\"YoutubeTranscriptReader\")\n",
        "\n",
        "loader = YoutubeTranscriptReader()\n",
        "documents = loader.load_data(ytlinks=['https://www.youtube.com/watch?v=K7Kh9Ntd8VE&ab_channel=DaveNick'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "0995d23b",
      "metadata": {
        "id": "0995d23b"
      },
      "outputs": [],
      "source": [
        "index = GPTSimpleVectorIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "id": "194e88fe",
      "metadata": {
        "scrolled": true,
        "id": "194e88fe",
        "outputId": "ade5b6f8-b1ac-44dc-ddfb-316a1d1c2984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>\nSome YouTube automation mistakes to avoid include: using automated bots to generate comments, using automated bots to generate likes or dislikes, using automated bots to generate views, using automated bots to generate subscribers, and using automated bots to generate shares. Additionally, automated bots should not be used to generate content, as this can lead to plagiarism and copyright infringement.</b>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = index.query(\"What some YouTube automation mistakes to avoid?\")\n",
        "#print(response)\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "359a05da",
      "metadata": {
        "id": "359a05da"
      },
      "source": [
        "# Chatbot Class - Just include your index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "65a0e830",
      "metadata": {
        "id": "65a0e830"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import json\n",
        "\n",
        "class Chatbot:\n",
        "    def __init__(self, api_key, index):\n",
        "        self.index = index\n",
        "        openai.api_key = api_key\n",
        "        self.chat_history = []\n",
        "\n",
        "    def generate_response(self, user_input):\n",
        "        prompt = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in self.chat_history[-5:]])\n",
        "        prompt += f\"\\nUser: {user_input}\"\n",
        "        response = index.query(user_input)\n",
        "\n",
        "        message = {\"role\": \"assistant\", \"content\": response.response}\n",
        "        self.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "        self.chat_history.append(message)\n",
        "        return message\n",
        "    \n",
        "    def load_chat_history(self, filename):\n",
        "        try:\n",
        "            with open(filename, 'r') as f:\n",
        "                self.chat_history = json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "\n",
        "    def save_chat_history(self, filename):\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(self.chat_history, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "a0d3da1c",
      "metadata": {
        "id": "a0d3da1c"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader('/content/drive/MyDrive/chatbot_knowledge/data').load_data()\n",
        "index = GPTSimpleVectorIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "id": "24576df3",
      "metadata": {
        "id": "24576df3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98adb777-4e3d-4720-bf74-b4f96a5f60fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: What is Llama?\n",
            "Bot: \n",
            "LLaMA is a state-of-the-art foundational large language model designed to help researchers advance their work in this subfield of AI. It is designed to be versatile and can be applied to many different use cases, and is trained on a large set of unlabeled data. It is being released under a noncommercial license focused on research use cases.\n",
            "You: bye\n",
            "Bot: Goodbye!\n"
          ]
        }
      ],
      "source": [
        "# Swap out your index below for whatever knowledge base you want\n",
        "bot = Chatbot(\"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\", index=index)\n",
        "bot.load_chat_history(\"chat_history.json\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"bye\", \"goodbye\"]:\n",
        "        print(\"Bot: Goodbye!\")\n",
        "        bot.save_chat_history(\"chat_history.json\")\n",
        "        break\n",
        "    response = bot.generate_response(user_input)\n",
        "    print(f\"Bot: {response['content']}\")\n",
        "    #display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ressources\n",
        "- [Llamaindex documentarion](https://gpt-index.readthedocs.io/en/latest/how_to/analysis/playground.html#sample-code)\n",
        "\n",
        "- [Llama hub](https://llamahub.ai/)"
      ],
      "metadata": {
        "id": "X-QXcawfWGcr"
      },
      "id": "X-QXcawfWGcr"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J0tWxTzpWRYD"
      },
      "id": "J0tWxTzpWRYD",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}