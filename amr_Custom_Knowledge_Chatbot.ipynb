{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/royam0820/ChatGPT-Google/blob/main/amr_Custom_Knowledge_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6337c0b"
      },
      "source": [
        "# Custom Knowledge Chatbot w/ LlamaIndex\n",
        "This notebook has been done via input from By Liam Ottley \n",
        "\n",
        "Github: https://github.com/wombyz/custom-knowledge-chatbot/tree/main/custom-knowledge-chatbot"
      ],
      "id": "a6337c0b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8911e71"
      },
      "source": [
        "Examples:\n",
        "- https://gita.kishans.in/\n",
        "- https://www.chatpdf.com/\n",
        "- https://www.chatbase.co/create-new-chatbot\n"
      ],
      "id": "a8911e71"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml6MMJFSOZOW",
        "outputId": "71a710b9-ec77-4de8-d1bd-0fb279dedbdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# connecting a Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "ml6MMJFSOZOW"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NSiDXMALP7A5"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/wombyz/custom-knowledge-chatbot.git"
      ],
      "id": "NSiDXMALP7A5"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c425f155",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40436e29-f42a-4de8-cf15-9e153e6e2ca4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/175.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.7/520.7 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama_index (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install llama_index -q # accessing vectorized data\n",
        "!pip install langchain -q # accessing OpenAI api\n",
        "from IPython.display import Markdown, display # for print formatting"
      ],
      "id": "c425f155"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hThsMxHyvERp"
      },
      "source": [
        "- **LlamaIndex** (GPT-Index) is a project that provides a central interface to connect your LLM's with external data. It will build a vector search on your data.  It offers data connectors to your existing data sources and data formats (API's, PDF's, docs, SQL, etc.) Provides indices over your unstructured and structured data for use with LLM's. https://gpt-index.readthedocs.io/en/latest/index.html\n",
        "\n",
        "- **LangChain** is a framework built around Large Language Models (LLMs) that can be used for various natural language processing tasks. At its core, LangChain allows developers to \"chain\" together different components to create more advanced use cases around LLMs. LangChain is a framework built around Large Language Models (LLMs) that can be used for various natural language processing tasks. At its core, LangChain allows developers to \"chain\" together different components to create more advanced use cases around LLMs  https://www.pinecone.io/learn/langchain-intro/"
      ],
      "id": "hThsMxHyvERp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1041eae8"
      },
      "source": [
        "# Basic LlamaIndex Usage Pattern\n",
        "\n",
        "- Get an OpenAI key\n",
        "- Load data into Documents, a custom type by Llamaindex\n",
        "- Create a vectorized index of your documents\n",
        "- Save the index as a json file\n",
        "- Load the index from the saved json file\n",
        "- Query your index."
      ],
      "id": "1041eae8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Document Example (one example)"
      ],
      "metadata": {
        "id": "a6nPfOyc12tO"
      },
      "id": "a6nPfOyc12tO"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ec6395b7"
      },
      "outputs": [],
      "source": [
        "# Getting the OpenAI api key\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = \"YOUR OPENAI KEY\""
      ],
      "id": "ec6395b7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DpxI9zTwJFu"
      },
      "source": [
        "https://platform.openai.com/account/api-keys"
      ],
      "id": "8DpxI9zTwJFu"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5cf41880"
      },
      "outputs": [],
      "source": [
        "# Load your data into 'Documents' a custom type by LlamaIndex\n",
        "from llama_index import SimpleDirectoryReader\n",
        "documents = SimpleDirectoryReader('/content/drive/MyDrive/chatbot_knowledge/data').load_data()"
      ],
      "id": "5cf41880"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "98df5bf6"
      },
      "outputs": [],
      "source": [
        "# Create a vectorized index of your documents\n",
        "from llama_index import GPTSimpleVectorIndex\n",
        "\n",
        "index = GPTSimpleVectorIndex.from_documents(documents)\n",
        "\n",
        "#amr\n",
        "# save the index to a file\n",
        "index.save_to_disk('/content/drive/MyDrive/chatbot_knowledge/data/output/index_simple_vector.json')\n",
        "\n",
        "# load the index from the saved file\n",
        "index = GPTSimpleVectorIndex.load_from_disk('/content/drive/MyDrive/chatbot_knowledge/data/output/index_simple_vector.json')\n"
      ],
      "id": "98df5bf6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqg4zk-L0KJ5"
      },
      "source": [
        "Based on the provided web search results, **GPTSimpleVectorIndex** is an indexing system used to store and retrieve embeddings generated by Large Language Models (LLMs) like GPT. The embeddings are created during the index construction phase and stored in a vectorized index. This index allows for efficient querying and finding relevant parts of data based on the similarity of the query and the data [1][2]. The GPTSimpleVectorIndex is one of several indexing systems available in the GPT Index project, including GPTChromaIndex and GPTTreeIndex"
      ],
      "id": "vqg4zk-L0KJ5"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "18731b6b",
        "outputId": "f2cc36b3-d122-4b4e-af53-746b0c269f9c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>\nFacebook's LLaMa appears to be a promising development in the field of AI. It is designed to help researchers advance their work in this subfield of AI by providing access to a state-of-the-art large language model. It is also designed to be versatile and can be applied to many different use cases, which is beneficial for researchers who don't have access to large amounts of infrastructure. Additionally, Facebook is taking responsible AI practices into consideration by providing a model card and releasing the model under a noncommercial license focused on research use cases.</b>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Query your index!\n",
        "\n",
        "response = index.query(\"What do you think of Facebook's LLaMa?\")\n",
        "#print(response)\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "id": "18731b6b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebea2acc"
      },
      "source": [
        "## Customer Support Example (several documents)"
      ],
      "id": "ebea2acc"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "19f396a9"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader('/content/drive/MyDrive/chatbot_knowledge/asos').load_data()"
      ],
      "id": "19f396a9"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cb30944e"
      },
      "outputs": [],
      "source": [
        "index = GPTSimpleVectorIndex.from_documents(documents)"
      ],
      "id": "cb30944e"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "946c44ef",
        "outputId": "e3c9cc8f-59f4-4eaa-8e0d-3a3a0e8c8b14"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>\nIn the United Arab Emirates, you have the option of signing up for ASOS Premier, which gives you free Standard and Express delivery all year round when you spend over 150 AED. It costs 200 AED and is valid on the order you purchase it on.</b>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = index.query(\"What premier service options do I have in the UAE?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))\n",
        "#print(response)"
      ],
      "id": "946c44ef"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF Example"
      ],
      "metadata": {
        "id": "LFA0jjUsyNUV"
      },
      "id": "LFA0jjUsyNUV"
    },
    {
      "cell_type": "code",
      "source": [
        "#downloading the Meta earnings pdf file\n",
        "# !wget -P \"/content/drive/MyDrive/pdf_docs\" https://s21.q4cdn.com/399680738/files/doc_financials/2022/q4/Meta-12.31.2022-Exhibit-99.1-FINAL.pdf "
      ],
      "metadata": {
        "id": "VyX-UbKLQxoS"
      },
      "execution_count": 22,
      "outputs": [],
      "id": "VyX-UbKLQxoS"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "J0tWxTzpWRYD"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from llama_index import download_loader\n",
        "\n",
        "PDFReader = download_loader(\"PDFReader\")\n",
        "\n",
        "\n",
        "loader = PDFReader()\n",
        "\n",
        "documents = loader.load_data(file=Path('/content/drive/MyDrive/chatbot_knowledge/pdf/Don’t Fear the Terminator.pdf'))"
      ],
      "id": "J0tWxTzpWRYD"
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import GPTSimpleVectorIndex\n",
        "\n",
        "index = GPTSimpleVectorIndex.from_documents(documents)\n",
        "\n",
        "#amr\n",
        "# save the index to a file\n",
        "index.save_to_disk('/content/drive/MyDrive/chatbot_knowledge/data/index_simple_vector_pdf.json')\n",
        "\n",
        "# load the index from the saved file\n",
        "index = GPTSimpleVectorIndex.load_from_disk('/content/drive/MyDrive/chatbot_knowledge/data/index_simple_vector_pdf.json')"
      ],
      "metadata": {
        "id": "IwXcEktF0R0R"
      },
      "id": "IwXcEktF0R0R",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query your index!\n",
        "\n",
        "response = index.query(\"Give us a summarization of the document?\")\n",
        "#print(response)\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "jHIw1y2M0dzs",
        "outputId": "ca7dd915-acd7-4755-88ba-a594b59af8fd"
      },
      "id": "jHIw1y2M0dzs",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>\nThis document discusses the risks and benefits of artificial intelligence (AI). It argues that the fear of AI taking over the world is misguided, as AI does not have a survival instinct and is not driven by a desire for dominance. Instead, AI is an evolutionary adaptation that can be used to achieve whatever goals humans set for it. The document also outlines the more likely risks posed by AI, such as economic disruption, weaponization, and unknown unknowns. It concludes that while AI has the potential to create a utopia, it is more likely to lead to further increases in wealth and income inequality.</b>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query your index!\n",
        "\n",
        "response = index.query(\"can you give us key points in a bullet list\")\n",
        "print(response)\n",
        "#display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEcqiucV06eS",
        "outputId": "5e12f01b-ac66-45c0-ec03-becaa75ed8ae"
      },
      "id": "zEcqiucV06eS",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "• AI systems did not pass through the crucible of natural selection, so they do not have a survival instinct. \n",
            "• Intelligence does not generate the drive for domination, it is just the ability to acquire and apply knowledge and skills in pursuit of a goal. \n",
            "• AI can be weaponized and may lead to new modes of warfare. \n",
            "• AI may disrupt much of our current economy, leading to job displacement and further increases in wealth and income inequalities. \n",
            "• Unknown risks are associated with any new technology. \n",
            "• At its best, AI has the potential to release us from mundane work and create a utopia. \n",
            "• At its worst, World War III might be fought by armies of superintelligent robots, but they will remain under our command.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query your index!\n",
        "\n",
        "response = index.query(\"who wrote this document and what year\")\n",
        "print(response)\n",
        "#display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWqjGTq21SCj",
        "outputId": "ab8b0dd4-0991-4c11-bf90-9fea2dd40a68"
      },
      "id": "qWqjGTq21SCj",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "This document was written by Anthony Zador and Yann LeCun in 2019.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "458f4e5c"
      },
      "source": [
        "## Wikipedia Example"
      ],
      "id": "458f4e5c"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4db9772b"
      },
      "outputs": [],
      "source": [
        "from llama_index import download_loader\n",
        "\n",
        "WikipediaReader = download_loader(\"WikipediaReader\")\n",
        "\n",
        "loader = WikipediaReader()\n",
        "wikidocs = loader.load_data(pages=['Cyclone Freddy'])"
      ],
      "id": "4db9772b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41WcT3tpIPNA"
      },
      "source": [
        "[Cyclone Freddy](https://en.wikipedia.org/wiki/Cyclone_Freddy)"
      ],
      "id": "41WcT3tpIPNA"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "941c9bf2"
      },
      "outputs": [],
      "source": [
        "wiki_index = GPTSimpleVectorIndex.from_documents(wikidocs)"
      ],
      "id": "941c9bf2"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "e4a4dd03",
        "outputId": "23e05055-b219-4dab-8c37-31ab9187c319"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>\n\nCyclone Freddy is a very intense tropical cyclone that traversed the southern Indian Ocean for more than five weeks in February and March 2023. It is both the longest-lasting and highest-ACE-producing tropical cyclone ever recorded worldwide. Freddy first developed as a disturbance on 5 February 2023 and quickly intensified to a Category 4 severe tropical cyclone before it moved into the South-West Indian Ocean basin, where it intensified further. The Joint Typhoon Warning Center (JTWC) estimated 1-minute sustained winds of 270 km/h (165 mph) at Freddy's peak strength, equivalent to Category 5 strength on the Saffir–Simpson scale. Freddy caused catastrophic damage in Madagascar, Mozambique, and Malawi, with at least 1,218 people killed in the storm. The cyclone also passed within 200 km (120 mi) of Mauritius, just north of Grand Bay, causing strong winds and waves along the northern coast of the island, with winds in Port Louis reaching 104 km/h (65 mph) and a peak gust of 154 km/h (96 mph) observed on Signal Mountain. Flooding and gale-force winds also affected the country, resulting in one fatality and at least 500 displaced families in a</b>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = wiki_index.query(\"What is cyclone freddy?\")\n",
        "#print(response)\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "id": "e4a4dd03"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "NbK26dBZb9pz",
        "outputId": "e90696a3-1fd9-4247-fc8c-17e384b6ef59"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>\n\nCyclone Freddy affected Madagascar, Mozambique, Malawi, Zimbabwe, South Africa, Eswatini, and Zambia. In Malawi, the Government of Malawi established 534 camps to house 508,244 displaced. In Mozambique, President Filipe Nyusi appealed for aid and to rebuild infrastructure, and provided MT250 million ($3.9 million) to Zambezia province to help restore everyday activities. The UN Humanitarian Coordinator for the nation, Martin Griffiths, released $10 million on 16 March to support cholera and emergency response aid. The WFP requires $26.7 million to assist 541,000 people impacted by Cyclone Freddy. As a result of flooding caused by the storm as well as the cutoff of access to water, sanitation, and hygiene services, cholera cases began to rapidly increase. At least 36 districts spanning 8 provinces are having outbreaks; the Inhambane and Zambezia provinces, which were heavily affected by Freddy, have declared outbreaks of cholera. The cumulative figure for the cases by 21 March stood at 11,158 across the provinces, and other waterborne illnesses such as diarrhea and malaria were of risk. The INGD did not have food and</b>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = wiki_index.query(\"What country were affected by cyclone freddy?\")\n",
        "#print(response)\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "id": "NbK26dBZb9pz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: \n",
        "- `max_input_size`: Maximum input size for the LLM.\n",
        "- `num_output`: Number of outputs for the LLM.\n",
        "- `max_chunk_overlap`: Maximum chunk overlap for the LLM. For instance, if you max_input_size is 1000 then the first sentence will have 100 characters and the next sentence will have 800 characters onward.\n"
      ],
      "metadata": {
        "id": "SQ6n6-b4VvFs"
      },
      "id": "SQ6n6-b4VvFs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e77e646"
      },
      "source": [
        "## YouTube Video Example"
      ],
      "id": "5e77e646"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "89160742"
      },
      "outputs": [],
      "source": [
        "YoutubeTranscriptReader = download_loader(\"YoutubeTranscriptReader\")\n",
        "\n",
        "loader = YoutubeTranscriptReader()\n",
        "documents = loader.load_data(ytlinks=['https://www.youtube.com/watch?v=K7Kh9Ntd8VE&ab_channel=DaveNick'])"
      ],
      "id": "89160742"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0995d23b"
      },
      "outputs": [],
      "source": [
        "index = GPTSimpleVectorIndex.from_documents(documents)"
      ],
      "id": "0995d23b"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "194e88fe",
        "outputId": "bd4717e7-027c-47cc-fa95-04a388bfa929",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>\n\n1. Re-uploading other people's content without permission, as this will get you in trouble and lead you nowhere.\n2. Not getting approved into the YouTube Partner Program, as this will prevent ads from running on your videos.\n3. Not promoting anything in the videos, as this will prevent you from making money with affiliate marketing or other revenue sources.\n4. Not leveraging the audience and channel to sell products.\n5. Not taking advantage of affiliate marketing or CPA marketing.\n6. Not creating exclusive memberships.\n7. Not using copyright free music, as this will get you in trouble if you use popular songs from artists.\n8. Not optimizing the video.\n9. Not writing a script.\n10. Not creating a thumbnail for each video.</b>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = index.query(\"What some YouTube automation mistakes to avoid?\")\n",
        "#print(response)\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "id": "194e88fe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SQL Example\n",
        "A separate notebook entitled `amr_langchain_sql` has been done."
      ],
      "metadata": {
        "id": "VfatMaEr47nH"
      },
      "id": "VfatMaEr47nH"
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index import download_loader\n",
        "\n",
        "# DatabaseReader = download_loader('DatabaseReader')\n",
        "\n",
        "# reader = DatabaseReader(\n",
        "#     scheme = \"postgresql\", # Database Scheme\n",
        "#     host = \"localhost\", # Database Host\n",
        "#     port = \"5432\", # Database Port\n",
        "#     user = \"postgres\", # Database User\n",
        "#     password = \"FakeExamplePassword\", # Database Password\n",
        "#     dbname = \"postgres\", # Database Name\n",
        "# )\n",
        "\n",
        "# query = f\"\"\"\n",
        "# SELECT\n",
        "#     CONCAT(name, ' is ', age, ' years old.') AS text\n",
        "# FROM public.users\n",
        "# WHERE age >= 18\n",
        "# \"\"\"\n",
        "\n",
        "# documents = reader.load_data(query=query)"
      ],
      "metadata": {
        "id": "t717-XOA56kc"
      },
      "id": "t717-XOA56kc",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57864389"
      },
      "source": [
        "# Customize your LLM for different output"
      ],
      "id": "57864389"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "duE1nkub4TDb"
      },
      "outputs": [],
      "source": [
        "from langchain import OpenAI"
      ],
      "id": "duE1nkub4TDb"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "c726dc51"
      },
      "outputs": [],
      "source": [
        "# Setup your LLM - Building the bot\n",
        "\n",
        "from llama_index import LLMPredictor, GPTSimpleVectorIndex, PromptHelper, ServiceContext\n",
        "\n",
        "\n",
        "# define LLM\n",
        "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.1, model_name=\"text-davinci-002\"))\n",
        "\n",
        "\n",
        "# define prompt helper\n",
        "#   set maximum input size\n",
        "max_input_size = 4096\n",
        "#   set number of output tokens\n",
        "num_output = 256\n",
        "#   set maximum chunk overlap\n",
        "max_chunk_overlap = 20\n",
        "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
        "\n",
        "# custom_LLM_index = GPTSimpleVectorIndex(\n",
        "#     documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper\n",
        "# )\n",
        "\n",
        "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n"
      ],
      "id": "c726dc51"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "359a05da"
      },
      "source": [
        "# Chatbot Class - Just include your index"
      ],
      "id": "359a05da"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "65a0e830"
      },
      "outputs": [],
      "source": [
        "# Defining a chatbot\n",
        "import openai\n",
        "import json\n",
        "\n",
        "class Chatbot:\n",
        "    def __init__(self, api_key, index):\n",
        "        self.index = index\n",
        "        openai.api_key = api_key\n",
        "        self.chat_history = []\n",
        "\n",
        "    def generate_response(self, user_input):\n",
        "        prompt = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in self.chat_history[-5:]])\n",
        "        prompt += f\"\\nUser: {user_input}\"\n",
        "        response = index.query(user_input)\n",
        "\n",
        "        message = {\"role\": \"assistant\", \"content\": response.response}\n",
        "        self.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "        self.chat_history.append(message)\n",
        "        return message\n",
        "    \n",
        "    def load_chat_history(self, filename):\n",
        "        try:\n",
        "            with open(filename, 'r') as f:\n",
        "                self.chat_history = json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "\n",
        "    def save_chat_history(self, filename):\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(self.chat_history, f)\n"
      ],
      "id": "65a0e830"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "a0d3da1c"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader('/content/drive/MyDrive/chatbot_knowledge/data').load_data()\n",
        "index = GPTSimpleVectorIndex.from_documents(documents)"
      ],
      "id": "a0d3da1c"
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = \"The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\\n\\nHuman: Hello, who are you?\\nAI: I am an AI created by OpenAI. How can I help you today?\\nHuman: \""
      ],
      "metadata": {
        "id": "5HpN0JtSoEI-"
      },
      "id": "5HpN0JtSoEI-",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "24576df3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "28228d86-91b1-4c9e-bdc8-cc6d1bed64e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: what is Llama?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>Bot: \nLLaMA is a state-of-the-art large language model designed to help researchers advance their work in the subfield of AI. It is a foundation model that is trained on a large set of unlabeled data, making it ideal for fine-tuning for a variety of tasks. LLaMA is available in several sizes (7B, 13B, 33B, and 65B parameters) and is released under a noncommercial license focused on research use cases.</b>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: bye\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>Bot: Goodbye!</b>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Swap out your index below for whatever knowledge base you want\n",
        "bot = Chatbot(\"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\", index=index)\n",
        "bot.load_chat_history(\"chat_history.json\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"bye\", \"goodbye\"]:\n",
        "        #print(\"Bot: Goodbye!\")\n",
        "        display(Markdown(f\"<b>Bot: Goodbye!</b>\"))\n",
        "        bot.save_chat_history(\"chat_history.json\")\n",
        "        break\n",
        "    response = bot.generate_response(user_input)\n",
        "    #print(f\"Bot: {response['content']}\")\n",
        "    display(Markdown(f\"<b>Bot: {response['content']}</b>\"))\n",
        "    "
      ],
      "id": "24576df3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGbJIz_AmHgI"
      },
      "source": [
        "# Gradio Interface for the Chatbot\n",
        "https://github.com/afizs/chatgpt-clone/blob/main/mini_ChatGPT.ipynb"
      ],
      "id": "FGbJIz_AmHgI"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "J6q6lG1ymkj7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cde8ac7-1c3e-495e-9e4f-4259e77fca94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.5/17.5 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.5/140.5 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.5/129.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio -q\n",
        "import gradio as gr"
      ],
      "id": "J6q6lG1ymkj7"
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_response(user_input):\n",
        "    if user_input.lower() in [\"bye\", \"goodbye\"]:\n",
        "        #print(\"Bot: Goodbye!\")\n",
        "        return \"Bot: Goodbye!\"\n",
        "    response = bot.generate_response(user_input)\n",
        "    #print(f\"Bot: {response['content']}\")\n",
        "    return f\"Bot: {response['content']}\"\n",
        "\n",
        "iface = gr.Interface(fn=chatbot_response, inputs=\"text\", outputs=\"text\", title=\"Chatbot\", description=\"Enter text to chat with the bot.\")\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "UNl_fcfq6Mz4",
        "outputId": "f67796f1-a190-4991-a209-83ad614ec5e1"
      },
      "id": "UNl_fcfq6Mz4",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"chat_history.json\", \"r\") as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inBLdzQ96-0n",
        "outputId": "94f70f2e-4162-40c2-f362-fa8aff366b0a"
      },
      "id": "inBLdzQ96-0n",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{\"role\": \"user\", \"content\": \"what is Llama?\"}, {\"role\": \"assistant\", \"content\": \"\\nLLaMA is a state-of-the-art large language model designed to help researchers advance their work in the subfield of AI. It is a foundation model that is trained on a large set of unlabeled data, making it ideal for fine-tuning for a variety of tasks. LLaMA is available in several sizes (7B, 13B, 33B, and 65B parameters) and is released under a noncommercial license focused on research use cases.\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-QXcawfWGcr"
      },
      "source": [
        "# Ressources\n",
        "\n",
        "- [Llamaindex documentarion](https://gpt-index.readthedocs.io/en/latest/how_to/analysis/playground.html#sample-code)\n",
        "\n",
        "- [Llama hub](https://llamahub.ai/) and [notebooks](https://github.com/jerryjliu/llama_index/tree/main/examples/data_connectors)\n",
        "\n",
        "- [Langchain](https://www.pinecone.io/learn/langchain-intro/#:~:text=At%20its%20core%2C%20LangChain%20is,advanced%20use%20cases%20around%20LLMs.)\n",
        "\n",
        "- [Chatgpt api and whisper api](https://openai.com/blog/introducing-chatgpt-and-whisper-apis)\n",
        "\n",
        "- [Openai Playground](https://platform.openai.com/playground)\n",
        "\n",
        "- [Openai community](https://community.openai.com/)\n",
        "\n",
        "- [Gutenberg.org for free ebooks](https://www.gutenberg.org/)\n",
        "\n",
        "- [Don't fear the terminator](https://blogs.scientificamerican.com/observations/dont-fear-the-terminator/)\n",
        "\n",
        "- [Openai Discord Channel](https://discord.com/channels/974519864045756446/1037561178286739466/1081542418714873929)\n"
      ],
      "id": "X-QXcawfWGcr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Glossary\n",
        "- **unstructured data**: If documents are not in a database or spreadsheet format, they’re “unstructured.” An “Unstructured Document” is a document that may contain valuable data, but the data is not organized in a fixed format. Consequently, it is difficult to “find” and capture the data for use. For instance, An email may contain important information, but it is presented in narrative text. Letters, emails, image files, text files, blogs, and social media posts are examples of unstructured documents.\n",
        "- **temperature**: Temperature is a value between 0 and 1 that essentially lets you control how confident the model should be when making these predictions. Lowering temperature means it will take fewer risks, and completions will be more accurate and deterministic. Increasing temperature will result in more diverse completions.\n",
        "- **maximum length**: Maximum length is a value between 1 to 4000. it is the maximum number of tokens that you allow your API call to generate. It’s just a cap: it doesn’t mean that your call will actually generate all these tokens. It should always be less than context_window - input_tokens, or you’ll get an error from OpenAI. In *text-davinci-003*, we have 4000 tokens, in *gpt-3.5-turbo* we have 2048 tokens\n",
        "and in *gpt-4* we have 2048 tokens.\n",
        "- **frequency penality**:  The frequency penality is a value between 0 to 2. the OpenAI Frequency Penalty setting is used to adjust how much frequency of tokens in the source material will influence the output of the model. With short (under 400 or so tokens) it’s not likely that frequency  will be noticeable… It’s when you have longer output that you start to see it in play.\n",
        "- **presence penality**: The presence penality is a value between 0 to 2. Presence penalty is a flat reduction if the token has appeared at least once before, while frequency penalty is bigger if the token has appeared multiple times.\n",
        "- **top P**: The value is between 0 and 1. It is a token sampling method. top_p computes the cumulative probability distribution, and cut off as soon as that distribution exceeds the value of top_p. For example, a top_p of 0.3 means that only the tokens comprising the top 30% probability mass are considered.\n"
      ],
      "metadata": {
        "id": "Pm97wqFPXa7z"
      },
      "id": "Pm97wqFPXa7z"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z4OSabrLirlt"
      },
      "id": "z4OSabrLirlt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dad Jokes"
      ],
      "metadata": {
        "id": "DYjnki6ZirqJ"
      },
      "id": "DYjnki6ZirqJ"
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import download_loader\n",
        "\n",
        "DadJokesReader = download_loader(\"DadJokesReader\")\n",
        "\n",
        "loader = DadJokesReader()\n",
        "documents = loader.load_data()"
      ],
      "metadata": {
        "id": "U2aJGDQrFWvD"
      },
      "id": "U2aJGDQrFWvD",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "nsxDw3kwFvwM"
      },
      "outputs": [],
      "source": [
        "# Create a vectorized index of your documents\n",
        "from llama_index import GPTSimpleVectorIndex\n",
        "\n",
        "index = GPTSimpleVectorIndex.from_documents(documents)\n",
        "\n",
        "#amr\n",
        "# save the index to a file\n",
        "index.save_to_disk('/content/drive/MyDrive/chatbot_knowledge/data/output/index_simple_vector.json')\n",
        "\n",
        "# load the index from the saved file\n",
        "index = GPTSimpleVectorIndex.load_from_disk('/content/drive/MyDrive/chatbot_knowledge/data/output/index_simple_vector.json')\n"
      ],
      "id": "nsxDw3kwFvwM"
    },
    {
      "cell_type": "code",
      "source": [
        "# Query your index!\n",
        "\n",
        "response = index.query(\"give me another one\")\n",
        "#print(response)\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "9jr9hH-0FgwN",
        "outputId": "baac2d31-9c12-4961-d061-e1042072a1be"
      },
      "id": "9jr9hH-0FgwN",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>\nNo matter how hard you try, you can't turn back time.</b>"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}