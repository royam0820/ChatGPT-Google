{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/royam0820/ChatGPT-Google/blob/main/update_amr_Custom_Knowledge_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6337c0b"
      },
      "source": [
        "# Custom Knowledge Chatbot w/ LlamaIndex\n",
        "This notebook has been done via input from By Liam Ottley\n",
        "\n",
        "Github: https://github.com/wombyz/custom-knowledge-chatbot/tree/main/custom-knowledge-chatbot\n",
        "\n",
        "This notebook has been revised on 05/2024 to take into account LangChain new revised package.\n",
        "\n",
        "- As per llama_index 0.8.40, `GPTSimpleVectorIndex` is deprecated and replaced by `GPTVectorStoreIndex`, and\n",
        "- To do a query, you need to change from `index.query` to\n",
        "\n",
        "\n",
        "```\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"My query\")\n",
        "```\n",
        "\n"
      ],
      "id": "a6337c0b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8911e71"
      },
      "source": [
        "Examples:\n",
        "- https://gita.kishans.in/\n",
        "- https://www.chatpdf.com/\n",
        "- https://www.chatbase.co/create-new-chatbot\n"
      ],
      "id": "a8911e71"
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = \"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\""
      ],
      "metadata": {
        "id": "fXEzfcOFH57v"
      },
      "execution_count": 169,
      "outputs": [],
      "id": "fXEzfcOFH57v"
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "ROY0FEd-TLNg"
      },
      "outputs": [],
      "source": [
        "# Getting the OpenAI api key\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
      ],
      "id": "ROY0FEd-TLNg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DpxI9zTwJFu"
      },
      "source": [
        "https://platform.openai.com/account/api-keys"
      ],
      "id": "8DpxI9zTwJFu"
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml6MMJFSOZOW",
        "outputId": "520a3adb-64ef-40ba-c2b7-1324913007cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# connecting a Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "ml6MMJFSOZOW"
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "NSiDXMALP7A5"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/wombyz/custom-knowledge-chatbot.git"
      ],
      "id": "NSiDXMALP7A5"
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "c425f155"
      },
      "outputs": [],
      "source": [
        "!pip install llama_index -q # accessing vectorized data\n",
        "!pip install langchain -q # accessing OpenAI api\n",
        "from IPython.display import Markdown, display # for print formatting"
      ],
      "id": "c425f155"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hThsMxHyvERp"
      },
      "source": [
        "- **LlamaIndex** (GPT-Index) is a project that provides a central interface to connect your LLM's with external data. It will build a vector search on your data.  It offers data connectors to your existing data sources and data formats (API's, PDF's, docs, SQL, etc.) Provides indices over your unstructured and structured data for use with LLM's. https://gpt-index.readthedocs.io/en/latest/index.html\n",
        "\n",
        "- **LangChain** is a framework built around Large Language Models (LLMs) that can be used for various natural language processing tasks. At its core, LangChain allows developers to \"chain\" together different components to create more advanced use cases around LLMs. LangChain is a framework built around Large Language Models (LLMs) that can be used for various natural language processing tasks. At its core, LangChain allows developers to \"chain\" together different components to create more advanced use cases around LLMs  https://www.pinecone.io/learn/langchain-intro/"
      ],
      "id": "hThsMxHyvERp"
    },
    {
      "cell_type": "code",
      "source": [
        "# llama index modules to import\n",
        "from llama_index.core import (\n",
        "    GPTVectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    StorageContext,\n",
        "    load_index_from_storage,\n",
        ")"
      ],
      "metadata": {
        "id": "B_qr9RDyvXKh"
      },
      "execution_count": 174,
      "outputs": [],
      "id": "B_qr9RDyvXKh"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xqvMo8r7WViT"
      },
      "id": "xqvMo8r7WViT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1041eae8"
      },
      "source": [
        "# Basic LlamaIndex Usage Pattern\n",
        "\n",
        "- Get an OpenAI key\n",
        "- Load data into Documents, a custom type by Llamaindex\n",
        "- Create a vectorized index of your documents\n",
        "- Save the index as a json file\n",
        "- Load the index from the saved json file\n",
        "- Query your index."
      ],
      "id": "1041eae8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Document Example (one example)"
      ],
      "metadata": {
        "id": "a6nPfOyc12tO"
      },
      "id": "a6nPfOyc12tO"
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "5cf41880"
      },
      "outputs": [],
      "source": [
        "# Load your data into 'Documents' a custom type by LlamaIndex\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "documents = SimpleDirectoryReader('/content/drive/MyDrive/chatbot_knowledge/data').load_data()"
      ],
      "id": "5cf41880"
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: As per the revised LangChain version the `SimpleDirectoryReader` must come from the package `llama_index.core`"
      ],
      "metadata": {
        "id": "W7OHlPMbXfZY"
      },
      "id": "W7OHlPMbXfZY"
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "98df5bf6"
      },
      "outputs": [],
      "source": [
        "# Create a vectorized index of your documents\n",
        "from llama_index.core import GPTVectorStoreIndex\n",
        "index = GPTVectorStoreIndex.from_documents(documents)"
      ],
      "id": "98df5bf6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqg4zk-L0KJ5"
      },
      "source": [
        "Based on the provided web search results, **GPTVectorStoreIndex** is an indexing system used to store and retrieve embeddings generated by Large Language Models (LLMs) like GPT. The embeddings are created during the index construction phase and stored in a vectorized index. This index allows for efficient querying and finding relevant parts of data based on the similarity of the query and the data [1][2]. The GPTVectorStoreIndex is one of several indexing systems available in the GPT Index project, including GPTChromaIndex and GPTTreeIndex"
      ],
      "id": "vqg4zk-L0KJ5"
    },
    {
      "cell_type": "code",
      "source": [
        "# # amr\n",
        "# # test on GPTVectorStoreIndex\n",
        "# from llama_index.core import StorageContext, load_index_from_disk\n",
        "\n",
        "# index.storage_context.persist(persist_dir=\"./storage\")\n",
        "\n",
        "# loaded_index = load_index_from_disk(StorageContext.from_defaults(persist_dir=\"./storage\"))"
      ],
      "metadata": {
        "id": "NwqpJJMwaz1G"
      },
      "id": "NwqpJJMwaz1G",
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index.storage_context.persist()"
      ],
      "metadata": {
        "id": "6riukWNrcxMm"
      },
      "id": "6riukWNrcxMm",
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Querying your index!\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What do you think of Facebook's LLaMa?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vtpLS4Sc5lN",
        "outputId": "2c0770d6-9a1f-4443-b1d3-3d1a64e82284"
      },
      "id": "1vtpLS4Sc5lN",
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Facebook's LLaMA is a state-of-the-art foundational large language model designed to help researchers advance their work in the field of AI. It is released as part of Meta's commitment to open science, aiming to democratize access to large language models by providing smaller, more performant models that require less computing power and resources. LLaMA is available in several sizes and is intended for fine-tuning for a variety of tasks. Additionally, Facebook emphasizes responsible AI practices by sharing a model card detailing how LLaMA was built and releasing the model under a noncommercial license for research purposes. The company encourages collaboration across the AI community to address challenges such as bias, toxicity, and hallucinations in large language models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "18731b6b",
        "outputId": "12287573-a5fb-4a61-f65f-02cdd8b2cd11"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>Facebook's LLaMA is a state-of-the-art foundational large language model designed to help researchers advance their work in the field of AI. It is released as part of Meta's commitment to open science, aiming to democratize access to large language models by providing smaller, more performant models that require less computing power and resources. LLaMA is available in several sizes and is intended for fine-tuning for a variety of tasks. Additionally, Facebook emphasizes responsible AI practices by sharing a model card detailing how LLaMA was built and releasing the model under a noncommercial license for research purposes. The company encourages collaboration across the AI community to address challenges such as bias, toxicity, and hallucinations in large language models.</b>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Displaying the response in bold, using markdown\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "id": "18731b6b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebea2acc"
      },
      "source": [
        "## Customer Support Example (several documents)"
      ],
      "id": "ebea2acc"
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "19f396a9"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader('/content/drive/MyDrive/chatbot_knowledge/asos').load_data()"
      ],
      "id": "19f396a9"
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "cb30944e"
      },
      "outputs": [],
      "source": [
        "index = GPTVectorStoreIndex.from_documents(documents)"
      ],
      "id": "cb30944e"
    },
    {
      "cell_type": "code",
      "source": [
        "index.storage_context.persist()"
      ],
      "metadata": {
        "id": "xzwfYMzgdi7i"
      },
      "execution_count": 183,
      "outputs": [],
      "id": "xzwfYMzgdi7i"
    },
    {
      "cell_type": "code",
      "source": [
        "# Querying your index!\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What premier service options do I have in the UAE?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab8f2848-24fd-44f0-f776-abcb9643714c",
        "id": "z9pQGp2Wdoll"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have the ASOS Premier service option available in the United Arab Emirates.\n"
          ]
        }
      ],
      "id": "z9pQGp2Wdoll"
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "bbb8944b-6dee-457a-d578-6d2169867432",
        "id": "Pc1eX03Qd0lr"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>You have the ASOS Premier service option available in the United Arab Emirates.</b>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Displaying the response in bold, using markdown\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "id": "Pc1eX03Qd0lr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF Example"
      ],
      "metadata": {
        "id": "LFA0jjUsyNUV"
      },
      "id": "LFA0jjUsyNUV"
    },
    {
      "cell_type": "code",
      "source": [
        "#downloading the Meta earnings pdf file\n",
        "# !wget -P \"/content/drive/MyDrive/pdf_docs\" https://s21.q4cdn.com/399680738/files/doc_financials/2022/q4/Meta-12.31.2022-Exhibit-99.1-FINAL.pdf"
      ],
      "metadata": {
        "id": "VyX-UbKLQxoS"
      },
      "execution_count": 186,
      "outputs": [],
      "id": "VyX-UbKLQxoS"
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "J0tWxTzpWRYD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ffa8eaf-2887-4bcf-b072-e876a5171ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-187-3f187b3f0e02>:4: DeprecationWarning: Call to deprecated function (or staticmethod) download_loader. (`download_loader()` is deprecated. Please install tool using pip install directly instead.)\n",
            "  PDFReader = download_loader(\"PDFReader\")\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 11 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 13 0 (offset 0)\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from llama_index.core import download_loader\n",
        "\n",
        "PDFReader = download_loader(\"PDFReader\")\n",
        "\n",
        "\n",
        "loader = PDFReader()\n",
        "\n",
        "documents = loader.load_data(file=Path('/content/drive/MyDrive/chatbot_knowledge/pdf/Don’t Fear the Terminator.pdf'))"
      ],
      "id": "J0tWxTzpWRYD"
    },
    {
      "cell_type": "code",
      "source": [
        "#from llama_index import GPTVectorStoreIndex\n",
        "index = GPTVectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "IwXcEktF0R0R"
      },
      "id": "IwXcEktF0R0R",
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index.storage_context.persist()"
      ],
      "metadata": {
        "id": "eQofGRnmfQmC"
      },
      "execution_count": 189,
      "outputs": [],
      "id": "eQofGRnmfQmC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Querying your index!\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"Give us a summarization of the document?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91605501-6e41-4126-e309-df7d3b91a8b0",
        "id": "rezjlkoxfQmD"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The document discusses the fear surrounding artificial intelligence (AI) potentially surpassing human intelligence and the concerns of AI taking over the world. It highlights that the idea of AI seeking dominance is a misconception rooted in human evolutionary history. The text emphasizes that AI does not inherently possess a survival instinct like biological organisms do. Instead, AI's intelligence can be directed towards goals set by humans. The document also mentions the more realistic risks posed by AI, such as economic disruptions, life-critical failures, and weaponization. Ultimately, it suggests that the focus should be on understanding and managing the practical risks of AI rather than worrying about a scenario where AI seeks to dominate humanity.\n"
          ]
        }
      ],
      "id": "rezjlkoxfQmD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Querying your index!\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"can you give us key points in a bullet list\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5251c69-a055-4151-8f39-285ebd556fbf",
        "id": "EIwezyW1feK2"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Isaac Asimov's First Law of Robotics: \"A robot may not injure a human being or, through inaction, allow a human being to come to harm.\"\n",
            "- Speculation on the link between intelligence and dominance in different species like elephants and orangutans.\n",
            "- Dominance-seeking behavior is more correlated with testosterone than with intelligence.\n",
            "- Concerns about AI being weaponized and disrupting the economy, potentially displacing jobs.\n",
            "- AI may lead to wealth and income inequalities unless new fiscal policies are implemented.\n",
            "- Risks associated with AI technology, including the \"unknown unknowns.\"\n",
            "- AI's potential to bring about profound transformations, from releasing humans from mundane work to the risk of World War III fought by superintelligent robots.\n",
            "- The misconception of AI takeover due to intelligence and drive for dominance.\n",
            "- Explanation of how natural intelligence in biological organisms differs from artificial intelligence in terms of survival instinct.\n",
            "- AI's intelligence can serve whatever goals humans set for it due to the decoupling of intelligence and survival in AI systems.\n"
          ]
        }
      ],
      "id": "EIwezyW1feK2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Query your index!\n",
        "response = query_engine.query(\"who wrote this document and what year\")\n",
        "print(response)\n",
        "#display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWqjGTq21SCj",
        "outputId": "0fef7141-05c3-4cf8-d525-b1789d91ab7c"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anthony Zador and Yann LeCun wrote the document in 2019.\n"
          ]
        }
      ],
      "id": "qWqjGTq21SCj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "458f4e5c"
      },
      "source": [
        "## Wikipedia Example"
      ],
      "id": "458f4e5c"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q wikipedia"
      ],
      "metadata": {
        "id": "L3MSMrJsgkWM"
      },
      "id": "L3MSMrJsgkWM",
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia"
      ],
      "metadata": {
        "id": "aM3sKrnOhSB1"
      },
      "id": "aM3sKrnOhSB1",
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing on a wikipedia page\n",
        "page = wikipedia.page(\"Cyclone Freddy\", auto_suggest=False)\n",
        "print(page.summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Jj6Sr-vhjly",
        "outputId": "9e213bf9-6e86-4d6d-9157-6a22873cbefa"
      },
      "id": "5Jj6Sr-vhjly",
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Very Intense Tropical Cyclone Freddy, also known as Severe Tropical Cyclone Freddy, was an exceptionally long-lived, powerful, and deadly tropical cyclone that traversed the southern Indian Ocean for more than five weeks in February and March 2023. Freddy is both the longest-lasting and highest-ACE-producing tropical cyclone ever recorded worldwide, traveling across the southern Indian Ocean, Mozambique, and Madagascar for 37 days and producing 87.01 units of ACE. Additionally, it is the third-deadliest tropical cyclone recorded in the Southern Hemisphere, only behind 2019's Cyclone Idai and the 1973 Flores cyclone. Freddy was the fourth named storm of the 2022–23 Australian region cyclone season, and the second very intense tropical cyclone of the 2022–23 South-West Indian Ocean cyclone season.\n",
            "Freddy first developed as a disturbance on 5 February 2023. While in the Australian region cyclone basin, the storm quickly intensified and became a Category 4 severe tropical cyclone, before it moved into the South-West Indian Ocean basin, where it intensified further. The Joint Typhoon Warning Center (JTWC) estimated 1-minute sustained winds of 270 km/h (165 mph) at Freddy's peak strength, equivalent to Category 5 strength on the Saffir–Simpson scale. On 19 February, Météo-France (MFR) upgraded it to a very intense tropical cyclone, estimated 10-minute winds of 220 km/h (140 mph). Freddy made its first landfall near Mananjary, Madagascar. The storm rapidly weakened overland but re-strengthened in the Mozambique Channel. Shortly afterward, Freddy made second landfall just south of Vilankulos, Mozambique, before rapidly weakening again. Unexpectedly, the system managed to survive its visit in Mozambique and emerged back over the channel on 1 March. Soon after, Freddy was re-classified as a tropical cyclone by the MFR. Over the course of 10 days, Freddy rapidly intensified on two occasions, eventually slowing to a semi-stationary movement near Quelimane, Mozambique. Moving northwest inland, the storm gradually deteriorated and was last noted on 14 March.\n",
            "Preparations for the storm in the Mascarene Islands included flight groundings, cyclone alerts, and personnel being prepped for the aftermath, among other things. In Madagascar, areas previously affected by Cyclones Batsirai and Cheneso were feared to be worsened by the storm's arrival. The cyclone struck southeastern Madagascar, damaging many homes. Impacts in Mozambique were more severe than in Madagascar and included heavy rainfall in the southern half of the country and widely damaged infrastructure. Effects in Mozambique were exacerbated after its second landfall with further floods and wind damage. The hardest-hit was Malawi where incessant rains caused catastrophic flash floods, especially Blantyre. The nation's power grid was crippled, with its hydroelectric dam rendered inoperable.  \n",
            "Overall, the cyclone killed at least 1,434 people: at least 1,216 people were killed in Malawi, 198 in Mozambique, 17 in Madagascar, 2 in Zimbabwe, and 1 in Mauritius, making it the first tropical cyclone globally to claim at least 1,000 lives since Cyclone Idai in 2019. Total damages are estimated to reach $655 million USD, making it the second-costliest cyclone in the South-West Indian Ocean after Idai.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's load the page and query it via LangChain"
      ],
      "metadata": {
        "id": "YqUE0UL9r_nI"
      },
      "id": "YqUE0UL9r_nI"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WikipediaLoader\n",
        "\n",
        "# Charger le contenu depuis Wikipedia en utilisant WikipediaLoader\n",
        "loader = WikipediaLoader(\"Machine_learning\")\n",
        "wikidocs = loader.load()"
      ],
      "metadata": {
        "id": "IQ_Mw7_Qin3I"
      },
      "id": "IQ_Mw7_Qin3I",
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(wikidocs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "440kZ82r_Ks7",
        "outputId": "7d9f9a07-9d32-464e-b71f-2829ab54e18e"
      },
      "id": "440kZ82r_Ks7",
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1qFUEBvAkl6"
      },
      "outputs": [],
      "source": [
        "# # Issue with GPTVectorStoreIndex and Wikipedia --- NOT SOLVED\n",
        "# index = GPTVectorStoreIndex.from_documents(\n",
        "#     wikidocs, storage_context=storage_context,\n",
        "#     service_context=service_context\n",
        "# )\n"
      ],
      "id": "A1qFUEBvAkl6"
    },
    {
      "cell_type": "code",
      "source": [
        "# wikidocs = str(list(wikidocs))"
      ],
      "metadata": {
        "id": "7y08CRV9_uyZ"
      },
      "id": "7y08CRV9_uyZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# type(wikidocs)"
      ],
      "metadata": {
        "id": "_cnNjC9D_1RH"
      },
      "id": "_cnNjC9D_1RH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wikidocs"
      ],
      "metadata": {
        "id": "EBm8D6MX_9sQ"
      },
      "id": "EBm8D6MX_9sQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "941c9bf2"
      },
      "outputs": [],
      "source": [
        "# wiki_index = GPTVectorStoreIndex.from_documents(wikidocs)"
      ],
      "id": "941c9bf2"
    },
    {
      "cell_type": "code",
      "source": [
        "# index.storage_context.persist()"
      ],
      "metadata": {
        "id": "b8yoOifIkOHh"
      },
      "execution_count": null,
      "outputs": [],
      "id": "b8yoOifIkOHh"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Query your index!\n",
        "# response = query_engine.query(\"what is Cyclone Freddy\")\n",
        "# print(response)\n",
        "# #display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "metadata": {
        "id": "JT_7ZTkTrfaj"
      },
      "execution_count": null,
      "outputs": [],
      "id": "JT_7ZTkTrfaj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4db9772b"
      },
      "outputs": [],
      "source": [
        "# from llama_index.core import download_loader\n",
        "\n",
        "# WikipediaReader = download_loader(\"WikipediaReader\")\n",
        "\n",
        "# loader = WikipediaReader()\n",
        "# #wikidocs = loader.load_data(pages=['Cyclone Freddy'])\n",
        "# wikidocs = loader.load_data(pages=page)"
      ],
      "id": "4db9772b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41WcT3tpIPNA"
      },
      "source": [
        "[Cyclone Freddy](https://en.wikipedia.org/wiki/Cyclone_Freddy)"
      ],
      "id": "41WcT3tpIPNA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4a4dd03"
      },
      "outputs": [],
      "source": [
        "# response = wiki_index.query(\"What is cyclone freddy?\")\n",
        "# #print(response)\n",
        "# display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "id": "e4a4dd03"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbK26dBZb9pz"
      },
      "outputs": [],
      "source": [
        "# response = wiki_index.query(\"What country were affected by cyclone freddy?\")\n",
        "# #print(response)\n",
        "# display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "id": "NbK26dBZb9pz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB:\n",
        "- `max_input_size`: Maximum input size for the LLM.\n",
        "- `num_output`: Number of outputs for the LLM.\n",
        "- `max_chunk_overlap`: Maximum chunk overlap for the LLM. For instance, if you max_input_size is 1000 then the first sentence will have 100 characters and the next sentence will have 800 characters onward.\n"
      ],
      "metadata": {
        "id": "SQ6n6-b4VvFs"
      },
      "id": "SQ6n6-b4VvFs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e77e646"
      },
      "source": [
        "## YouTube Video Example"
      ],
      "id": "5e77e646"
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "89160742",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6a2ca3d-a4fe-4556-c325-46d9a13a17cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-200-86675a688b3d>:1: DeprecationWarning: Call to deprecated function (or staticmethod) download_loader. (`download_loader()` is deprecated. Please install tool using pip install directly instead.)\n",
            "  YoutubeTranscriptReader = download_loader(\"YoutubeTranscriptReader\")\n"
          ]
        }
      ],
      "source": [
        "YoutubeTranscriptReader = download_loader(\"YoutubeTranscriptReader\")\n",
        "\n",
        "loader = YoutubeTranscriptReader()\n",
        "documents = loader.load_data(ytlinks=['https://www.youtube.com/watch?v=K7Kh9Ntd8VE&ab_channel=DaveNick'])"
      ],
      "id": "89160742"
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "0995d23b"
      },
      "outputs": [],
      "source": [
        "# Indexing the documents\n",
        "index = GPTVectorStoreIndex.from_documents(documents)"
      ],
      "id": "0995d23b"
    },
    {
      "cell_type": "code",
      "source": [
        "index.storage_context.persist()"
      ],
      "metadata": {
        "id": "1Yn8b9cJDmef"
      },
      "execution_count": 202,
      "outputs": [],
      "id": "1Yn8b9cJDmef"
    },
    {
      "cell_type": "code",
      "source": [
        "# Querying your index!\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"Whare are some YouTube automation mistakes to avoid\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c01bab14-5c75-44f3-f991-0bad886b6f0d",
        "id": "Y57DqnIODmeg"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avoid the mistake of thinking that YouTube automation involves stealing someone else's content and reposting it on your channel for profit. This is illegal and not a legitimate way to run a YouTube automation business. It's important to understand the right way to create and manage a YouTube automation system to build a sustainable online business. Additionally, a common mistake to avoid is not properly setting up your automation systems to get views, subscribers, and ultimately make money. It's crucial to follow the correct steps and strategies to ensure the success of your YouTube automation business.\n"
          ]
        }
      ],
      "id": "Y57DqnIODmeg"
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "194e88fe",
        "outputId": "115ce66e-b6ab-4cfc-e112-b700938062e2",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>Avoid the mistake of thinking that YouTube automation involves stealing someone else's content and reposting it on your channel for profit. This is illegal and not a legitimate way to run a YouTube automation business. It's important to understand the right way to create and manage a YouTube automation system to build a sustainable online business. Additionally, a common mistake to avoid is not properly setting up your automation systems to get views, subscribers, and ultimately make money. It's crucial to follow the correct steps and strategies to ensure the success of your YouTube automation business.</b>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#print(response)\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "id": "194e88fe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SQL Example\n",
        "A separate notebook entitled `amr_langchain_sql` has been done."
      ],
      "metadata": {
        "id": "VfatMaEr47nH"
      },
      "id": "VfatMaEr47nH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57864389"
      },
      "source": [
        "# Customize your LLM for different output"
      ],
      "id": "57864389"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index-llms-langchain"
      ],
      "metadata": {
        "id": "CYxiTMkMLb4o"
      },
      "id": "CYxiTMkMLb4o",
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "duE1nkub4TDb"
      },
      "outputs": [],
      "source": [
        "from langchain import OpenAI\n",
        "from llama_index.core import llms, GPTVectorStoreIndex, PromptHelper, ServiceContext\n"
      ],
      "id": "duE1nkub4TDb"
    },
    {
      "cell_type": "code",
      "source": [
        "# indicate which model to use\n",
        "model = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.1)"
      ],
      "metadata": {
        "id": "TeI6QvDhIfyU"
      },
      "id": "TeI6QvDhIfyU",
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define prompt helper\n",
        "#   set maximum input size\n",
        "max_input_size = 4096\n",
        "#   set number of output tokens\n",
        "num_output = 256\n",
        "#   set maximum chunk overlap\n",
        "max_chunk_overlap = 1.\n",
        "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
        "\n",
        "# custom_LLM_index = GPTVectorStoreIndex(\n",
        "#     documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper\n",
        "# )\n",
        "\n",
        "#service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
        "service_context = ServiceContext.from_defaults(llm = model, prompt_helper=prompt_helper)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTW4p8HWJxHR",
        "outputId": "4a9d9758-5e04-46d6-c6b0-bfc7830658ae"
      },
      "id": "rTW4p8HWJxHR",
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-208-bc7c1705ed55>:15: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(llm = model, prompt_helper=prompt_helper)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "c726dc51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "236bef4f-5af6-4feb-a4bb-146627648ce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-209-e8c147867405>:27: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(llm=model, prompt_helper=prompt_helper)\n"
          ]
        }
      ],
      "source": [
        "# Setup your LLM - Building the bot\n",
        "\n",
        "#from llama_index.core import LLMPredictor, GPTVectorStoreIndex, PromptHelper, ServiceContext\n",
        "from llama_index.core import llms, GPTVectorStoreIndex, PromptHelper, ServiceContext\n",
        "\n",
        "\n",
        "# define LLM\n",
        "model = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.1)\n",
        "\n",
        "\n",
        "#llm_predictor = llms(llm=OpenAI(temperature=0.1, model_name=\"text-davinci-002\"))\n",
        "\n",
        "\n",
        "# define prompt helper\n",
        "#   set maximum input size\n",
        "max_input_size = 4096\n",
        "#   set number of output tokens\n",
        "num_output = 256\n",
        "#   set maximum chunk overlap\n",
        "max_chunk_overlap = 1.0\n",
        "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
        "\n",
        "# custom_LLM_index = GPTVectorStoreIndex(\n",
        "#     documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper\n",
        "# )\n",
        "\n",
        "service_context = ServiceContext.from_defaults(llm=model, prompt_helper=prompt_helper)\n"
      ],
      "id": "c726dc51"
    },
    {
      "cell_type": "code",
      "source": [
        "llm.model_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LYhaN4P5Ik7d",
        "outputId": "ab7a043a-3ee2-47d6-c556-f80bbfa9d3c5"
      },
      "id": "LYhaN4P5Ik7d",
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gpt-3.5-turbo-instruct'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.temperature"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TF4vLAyfI76O",
        "outputId": "bc53f2e0-f6d7-4f0d-e2b7-b319d5240f25"
      },
      "id": "TF4vLAyfI76O",
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "359a05da"
      },
      "source": [
        "# Chatbot Class - Just include your index"
      ],
      "id": "359a05da"
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "65a0e830"
      },
      "outputs": [],
      "source": [
        "# Defining a chatbot\n",
        "import openai\n",
        "import json\n",
        "\n",
        "class Chatbot:\n",
        "    def __init__(self, api_key, index):\n",
        "        self.index = index\n",
        "        openai.api_key = api_key\n",
        "        self.chat_history = []\n",
        "\n",
        "    def generate_response(self, user_input):\n",
        "        prompt = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in self.chat_history[-5:]])\n",
        "        prompt += f\"\\nUser: {user_input}\"\n",
        "        #response = index.query(user_input)\n",
        "        response = query_engine.query(user_input)\n",
        "\n",
        "        message = {\"role\": \"assistant\", \"content\": response.response}\n",
        "        self.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "        self.chat_history.append(message)\n",
        "        return message\n",
        "\n",
        "    def load_chat_history(self, filename):\n",
        "        try:\n",
        "            with open(filename, 'r') as f:\n",
        "                self.chat_history = json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "\n",
        "    def save_chat_history(self, filename):\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(self.chat_history, f)\n"
      ],
      "id": "65a0e830"
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "f9Bg0t_ePT_J"
      },
      "outputs": [],
      "source": [
        "# Load your data into 'Documents' a custom type by LlamaIndex\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "documents = SimpleDirectoryReader('/content/drive/MyDrive/chatbot_knowledge/data').load_data()"
      ],
      "id": "f9Bg0t_ePT_J"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vectorized index of your documents\n",
        "from llama_index.core import GPTVectorStoreIndex\n",
        "index = GPTVectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "kS4Ux8I9P0IM"
      },
      "id": "kS4Ux8I9P0IM",
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index.storage_context.persist()"
      ],
      "metadata": {
        "id": "7Qbr0PvaMM5c"
      },
      "execution_count": 215,
      "outputs": [],
      "id": "7Qbr0PvaMM5c"
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = \"The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\\n\\nHuman: Hello, who are you?\\nAI: I am an AI created by OpenAI. How can I help you today?\\nHuman: \""
      ],
      "metadata": {
        "id": "5HpN0JtSoEI-"
      },
      "id": "5HpN0JtSoEI-",
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Querying your index!\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What do you think of Facebook's LLaMa?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f345c908-db8a-4cd5-8e6c-9ad751cc7cad",
        "id": "cnSe7F3FQSwV"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Facebook's LLaMA is a state-of-the-art foundational large language model designed to support researchers in the field of AI. It is created to be versatile and applicable to various tasks, unlike fine-tuned models that are specific to particular functions. The model is trained on a large set of unlabeled data and is available in different sizes to facilitate research and experimentation in the AI community. Additionally, Facebook is committed to responsible AI practices and has released LLaMA under a noncommercial license for research purposes, aiming to address challenges such as bias, toxicity, and misinformation in large language models.\n"
          ]
        }
      ],
      "id": "cnSe7F3FQSwV"
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "24576df3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "b8b5c26e-62c5-4468-9786-1b04511a1579"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: What is Llama?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>Bot: LLaMA (Large Language Model Meta AI) is a state-of-the-art foundational large language model designed to assist researchers in advancing their work in the field of AI. It is available in several sizes (7B, 13B, 33B, and 65B parameters) and is intended to be versatile for various tasks. LLaMA works by predicting the next word in a sequence of words to generate text recursively. It is trained on a large set of unlabeled data and aims to democratize access to large language models by requiring less computing power and resources compared to larger models.</b>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: bye\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>Bot: Goodbye!</b>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Swap out your index below for whatever knowledge base you want\n",
        "bot = Chatbot(\"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\", index=index)\n",
        "bot.load_chat_history(\"chat_history.json\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"bye\", \"goodbye\"]:\n",
        "        #print(\"Bot: Goodbye!\")\n",
        "        display(Markdown(f\"<b>Bot: Goodbye!</b>\"))\n",
        "        bot.save_chat_history(\"chat_history.json\")\n",
        "        break\n",
        "    response = bot.generate_response(user_input)\n",
        "    #print(f\"Bot: {response['content']}\")\n",
        "    display(Markdown(f\"<b>Bot: {response['content']}</b>\"))\n"
      ],
      "id": "24576df3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGbJIz_AmHgI"
      },
      "source": [
        "# Gradio Interface for the Chatbot\n",
        "https://github.com/afizs/chatgpt-clone/blob/main/mini_ChatGPT.ipynb"
      ],
      "id": "FGbJIz_AmHgI"
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "J6q6lG1ymkj7"
      },
      "outputs": [],
      "source": [
        "!pip install gradio -q\n",
        "import gradio as gr"
      ],
      "id": "J6q6lG1ymkj7"
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_response(user_input):\n",
        "    if user_input.lower() in [\"bye\", \"goodbye\"]:\n",
        "        #print(\"Bot: Goodbye!\")\n",
        "        return \"Bot: Goodbye!\"\n",
        "    response = bot.generate_response(user_input)\n",
        "    #print(f\"Bot: {response['content']}\")\n",
        "    return f\"Bot: {response['content']}\"\n",
        "\n",
        "iface = gr.Interface(fn=chatbot_response, inputs=\"text\", outputs=\"text\", title=\"Chatbot\", description=\"Enter text to chat with the bot.\")\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "UNl_fcfq6Mz4",
        "outputId": "9372df3f-96a9-4a12-febf-61de71658c20"
      },
      "id": "UNl_fcfq6Mz4",
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://a1b152803ddcfe4a88.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a1b152803ddcfe4a88.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"chat_history.json\", \"r\") as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inBLdzQ96-0n",
        "outputId": "f6710bcd-797f-43e2-e576-0eeb7c1f8be4"
      },
      "id": "inBLdzQ96-0n",
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{\"role\": \"user\", \"content\": \"what is Llama?\"}, {\"role\": \"assistant\", \"content\": \"I cannot provide an answer to the query as there is no mention of \\\"Llama\\\" in the provided context information.\"}, {\"role\": \"user\", \"content\": \"buy\"}, {\"role\": \"assistant\", \"content\": \"You can earn commissions through affiliate marketing by promoting products in your niche and including affiliate links in your video descriptions. This allows you to earn a percentage of the sale whenever someone makes a purchase through your link.\"}, {\"role\": \"user\", \"content\": \"youtube infos\"}, {\"role\": \"assistant\", \"content\": \"To optimize a YouTube channel, it is important to set up a logo or profile picture, channel art, and an informative about section with relevant keywords related to the videos being created. Utilizing tools like Photoshop or free alternatives such as Photopia or Canva can help in creating logos and channel art. Additionally, tools like VidIQ can assist in boosting YouTube views and generating video ideas. Successful YouTube automation channels, like Academy of Ideas and Elon Musk Rewind, demonstrate how channels can be monetized without showing the creator's face through voiceovers, scripts, and stock footage.\"}, {\"role\": \"user\", \"content\": \"what is Llama\"}, {\"role\": \"assistant\", \"content\": \"Llama is not mentioned in the provided context information.\"}, {\"role\": \"user\", \"content\": \"what is Llama\"}, {\"role\": \"assistant\", \"content\": \"Llama\"}, {\"role\": \"user\", \"content\": \"can you please tell me more\"}, {\"role\": \"assistant\", \"content\": \"You can start monetizing your YouTube channel through various revenue sources such as super chat and super stickers, print on demand products, brand deals or sponsorships, selling your own products (digital or physical), and utilizing voiceovers, video editing, and creating thumbnails to enhance your content. These strategies can help you generate income and grow your channel effectively.\"}, {\"role\": \"user\", \"content\": \"What is Llama?\"}, {\"role\": \"assistant\", \"content\": \"LLaMA (Large Language Model Meta AI) is a state-of-the-art foundational large language model designed to assist researchers in advancing their work in the field of AI. It is available in several sizes (7B, 13B, 33B, and 65B parameters) and is intended to be versatile for various tasks. LLaMA works by predicting the next word in a sequence of words to generate text recursively. It is trained on a large set of unlabeled data and aims to democratize access to large language models by requiring less computing power and resources compared to larger models.\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-QXcawfWGcr"
      },
      "source": [
        "# Ressources\n",
        "\n",
        "- [Llamaindex documentarion](https://gpt-index.readthedocs.io/en/latest/how_to/analysis/playground.html#sample-code)\n",
        "\n",
        "- [Llama hub](https://llamahub.ai/) and [notebooks](https://github.com/jerryjliu/llama_index/tree/main/examples/data_connectors)\n",
        "\n",
        "- [Langchain](https://www.pinecone.io/learn/langchain-intro/#:~:text=At%20its%20core%2C%20LangChain%20is,advanced%20use%20cases%20around%20LLMs.)\n",
        "\n",
        "- [Chatgpt api and whisper api](https://openai.com/blog/introducing-chatgpt-and-whisper-apis)\n",
        "\n",
        "- [Openai Playground](https://platform.openai.com/playground)\n",
        "\n",
        "- [Openai community](https://community.openai.com/)\n",
        "\n",
        "- [Gutenberg.org for free ebooks](https://www.gutenberg.org/)\n",
        "\n",
        "- [Don't fear the terminator](https://blogs.scientificamerican.com/observations/dont-fear-the-terminator/)\n",
        "\n",
        "- [Openai Discord Channel](https://discord.com/channels/974519864045756446/1037561178286739466/1081542418714873929)\n"
      ],
      "id": "X-QXcawfWGcr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Glossary\n",
        "- **unstructured data**: If documents are not in a database or spreadsheet format, they’re “unstructured.” An “Unstructured Document” is a document that may contain valuable data, but the data is not organized in a fixed format. Consequently, it is difficult to “find” and capture the data for use. For instance, An email may contain important information, but it is presented in narrative text. Letters, emails, image files, text files, blogs, and social media posts are examples of unstructured documents.\n",
        "- **temperature**: Temperature is a value between 0 and 1 that essentially lets you control how confident the model should be when making these predictions. Lowering temperature means it will take fewer risks, and completions will be more accurate and deterministic. Increasing temperature will result in more diverse completions.\n",
        "- **maximum length**: Maximum length is a value between 1 to 4000. it is the maximum number of tokens that you allow your API call to generate. It’s just a cap: it doesn’t mean that your call will actually generate all these tokens. It should always be less than context_window - input_tokens, or you’ll get an error from OpenAI. In *text-davinci-003*, we have 4000 tokens, in *gpt-3.5-turbo* we have 2048 tokens\n",
        "and in *gpt-4* we have 2048 tokens.\n",
        "- **frequency penality**:  The frequency penality is a value between 0 to 2. the OpenAI Frequency Penalty setting is used to adjust how much frequency of tokens in the source material will influence the output of the model. With short (under 400 or so tokens) it’s not likely that frequency  will be noticeable… It’s when you have longer output that you start to see it in play.\n",
        "- **presence penality**: The presence penality is a value between 0 to 2. Presence penalty is a flat reduction if the token has appeared at least once before, while frequency penalty is bigger if the token has appeared multiple times.\n",
        "- **top P**: The value is between 0 and 1. It is a token sampling method. top_p computes the cumulative probability distribution, and cut off as soon as that distribution exceeds the value of top_p. For example, a top_p of 0.3 means that only the tokens comprising the top 30% probability mass are considered.\n"
      ],
      "metadata": {
        "id": "Pm97wqFPXa7z"
      },
      "id": "Pm97wqFPXa7z"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z4OSabrLirlt"
      },
      "id": "z4OSabrLirlt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dad Jokes"
      ],
      "metadata": {
        "id": "DYjnki6ZirqJ"
      },
      "id": "DYjnki6ZirqJ"
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import download_loader\n",
        "\n",
        "DadJokesReader = download_loader(\"DadJokesReader\")\n",
        "\n",
        "loader = DadJokesReader()\n",
        "documents = loader.load_data()"
      ],
      "metadata": {
        "id": "U2aJGDQrFWvD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fd0204f-66eb-43db-e25f-a435a29067bb"
      },
      "id": "U2aJGDQrFWvD",
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-228-131f25b8dc78>:3: DeprecationWarning: Call to deprecated function (or staticmethod) download_loader. (`download_loader()` is deprecated. Please install tool using pip install directly instead.)\n",
            "  DadJokesReader = download_loader(\"DadJokesReader\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "nsxDw3kwFvwM"
      },
      "outputs": [],
      "source": [
        "# Create a vectorized index of your documents\n",
        "from llama_index.core import GPTVectorStoreIndex\n",
        "\n",
        "index = GPTVectorStoreIndex.from_documents(documents)\n",
        "\n"
      ],
      "id": "nsxDw3kwFvwM"
    },
    {
      "cell_type": "code",
      "source": [
        "# Query your index!\n",
        "query_engine.query\n",
        "response = query_engine.query(\"give me another one\")\n",
        "#print(response)\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "9jr9hH-0FgwN",
        "outputId": "5903303e-b663-4157-97e0-c634d59487c6"
      },
      "id": "9jr9hH-0FgwN",
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>LLaMA 13B is another model that has been trained on a significant amount of tokens, specifically 1.4 trillion tokens.</b>"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}